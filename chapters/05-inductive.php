	<section id="chap-inductive" class="chapter">
		<h2>Inductive Knowledge</h2>
		<p>While deductive knowledge is characterised by precise logical consequences, inductively acquiring knowledge involves generalising patterns from a given set of input observations, which can then be used to generate novel but potentially imprecise predictions. For example, from a large data graph with geographical and flight information, we may observe the pattern that almost all capital cities of countries have international airports serving them, and hence predict that if Santiago is a capital city, it <em>likely</em> has an international airport serving it; however, the predictions drawn from this pattern do not hold for certain, where (e.g.) Vaduz, the capital city of Liechtenstein, has no (international) airport serving it. Hence predictions will often be associated with a level of confidence; for example, we may say that a capital has an international airport in \(\frac{187}{195}\) of cases, offering a confidence of \(0.959\) for predictions made with that pattern. We then refer to knowledge acquired inductively as <em>inductive knowledge</em>, which includes both the models used to encode patterns, as well as the predictions made by those models. Though fallible, inductive knowledge can be highly valuable.</p>

		<figure id="fig-ind">
			<img src="images/fig-ind.svg" alt="Conceptual overview of popular inductive techniques for knowledge graphs in terms of type of representation generated (Numeric/Symbolic) and type of paradigm used (Unsupervised/Self-supervised/Supervised)"/>
			<figcaption>Conceptual overview of popular inductive techniques for knowledge graphs in terms of type of representation generated (Numeric/Symbolic) and type of paradigm used (Unsupervised/Self-supervised/Supervised)</figcaption>
		</figure>

		<p>In Figure&nbsp;<?php echo ref("fig:ind"); ?> we provide an overview of the inductive techniques typically applied to knowledge graphs. In the case of unsupervised methods, there is a rich body of work on <em>graph analytics</em>, which uses well-known functions/algorithms to detect communities or clusters, find central nodes and edges, etc., in a graph. Alternatively, <em>knowledge graph embeddings</em> can use self-supervision to learn a low-dimensional numeric model of a knowledge graph that (typically) maps input edges to an output <em>plausibility score</em> indicating the likelihood of the edge being true. The structure of graphs can also be directly leveraged for supervised learning, as explored in the context of <em>graph neural networks</em>. Finally, while the aforementioned techniques learn numerical models, <em>symbolic learning</em> can learn symbolic models – i.e., logical formulae in the form of rules or axioms – from a graph in a self-supervised manner. We now discuss each of the aforementioned techniques in turn.</p>

		<section id="sec-gAnalytics" class="section">
		<h3>Graph Analytics</h3>
		<p>Analytics is the process of discovering, interpreting, and communicating meaningful patterns inherent to (typically large) data collections. Graph analytics is then the application of analytical processes to (typically large) graph data. The nature of graphs naturally lends itself to certain types of analytics that derive conclusions about nodes and edges based on the <em>topology</em> of the graph, i.e., how the nodes of the graph are connected. Graph analytics draws upon techniques from related areas, such as graph theory and network analysis, which have been used to study graphs representing social networks, the Web, internet routing, transport networks, ecosystems, protein–protein interactions, linguistic cooccurrences, and more besides&nbsp;<?php echo $references->cite("Estrada2011"); ?>.</p>
		<p>Returning to the domain of our running example, the tourism board could use graph analytics to extract knowledge about, for instance: key transport hubs that serve many tourist attractions (centrality); groupings of attractions visited by the same tourists (community detection); attractions that may become unreachable in the event of strikes or other route failures (connectivity), or pairs of attractions that are similar to each other (node similarity). Given that such analytics will require a complex, large-scale graph, for the purposes of illustration, in Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> we present a more concise example of some transportation connections in Chile directed towards popular tourist destinations. We first introduce a selection of key techniques that can be applied for graph analytics. We then discuss frameworks and languages that can be used to compute such analytics in practice. Given that many traditional graph algorithms are defined for unlabelled graphs, we then describe ways in which analytics can be applied over directed edge-labelled graphs. Finally we discuss the potential connections between graph analytics and querying and reasoning.</p>

		<figure id="fig-chileTransport">
			<img src="images/fig-chileTransport.svg" alt="Data graph representing transport routes in Chile"/>
			<figcaption>Data graph representing transport routes in Chile <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_1_Graph_Analytics/figure_5_2.ttl"></a></figcaption>
		</figure>

		<h4 id="sssec-graph-analytics-tasks" class="subsection">Techniques</h4>
		<p>A wide variety of techniques can be applied for graph analytics. In the following we will enumerate some of the main techniques – as recognised, for example, by the survey of <?php echo $references->citet("IosupHNHPMCCSAT16"); ?> – that can be invoked in this setting.</p>
		<ul>
			<li><em>Centrality:</em> aims to identify the most important (aka <em>central</em>) nodes or edges of a graph. Specific node centrality measures include <em>degree</em>, <em>betweenness</em>, <em>closeness</em>, <em>Eigenvector</em>, <em>PageRank</em>, <em>HITS</em>, <em>Katz</em>, amongst others. Betweenness centrality can also be applied to edges. For example, a node centrality measure might predict the transport hubs in Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>, while edge centrality might predict traffic by finding connections on which many shortest routes depend.</li>
			<li><em>Community detection:</em> aims to identify <em>communities</em> in a graph, i.e., sub-graphs that are more densely connected internally than to the rest of the graph. Community detection algorithms, such as <em>minimum-cut algorithms</em>, <em>label propagation</em>, <em>Louvain modularity</em>, amongst others, can discover such communities. Community detection applied to Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> may, for example, detect a community to the left (the north of Chile), to the right (the south of Chile), and perhaps also the centre (Chilean cities with airports).</li>
			<li><em>Connectivity:</em> aims to estimate how well-connected the graph is, revealing, for instance, the resilience and (un)reachability of elements of the graph. Specific techniques include measuring <em>graph density</em> or <em>\(k\)-connectivity</em>, detecting <em>strongly connected components</em> and <em>weakly connected components</em>, computing <em>spanning trees</em> or <em>minimum cuts</em>, etc. In the context of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>, such analysis may tell us that routes to <span class="gnode">Grey Glacier</span>, <span class="gnode">Osorno Volcano</span> and <span class="gnode">Piedras Rojas</span> are the most “brittle”, becoming disconnected if one of two <span class="gelab">bus</span> routes fails.</li>
			<li><em>Node (or vertex) similarity:</em> aims to find nodes that are similar to other nodes by virtue of how they are connected within their neighbourhood. Node similarity metrics may be computed using <em>structural equivalence</em>, <em>random walks</em>, <em>diffusion kernels</em>, etc. These methods provide an understanding of what connects nodes, and, thereafter, in what ways they are similar. In the context of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>, such analysis may tell us that <span class="gnode">Calama</span> and <span class="gnode">Arica</span> are similar nodes based on both having return flights to <span class="gnode">Santiago</span> and return buses to <span class="gnode">San Pedro</span>.</li>
		</ul>
		<p>While the previous techniques accept a graph alone as input,<?php echo footnote("Node similarity can be run over an entire graph to find the \(k\) most similar nodes for each node, or can also be run for a specific node to find its most similar nodes. There are also measures for graph similarity (based on, e.g., frequent itemsets&nbsp;". $references->cite("MaillotB18"). ") that accept multiple graphs as input."); ?> other forms of graph analytics may further accept a node, a pair of nodes, etc., along with the graph.</p>
		<ul>
			<li><em>Path finding:</em> aims to find paths in a graph, typically between pairs of nodes given as input. Various technical definitions exist that restrict the set of valid paths between such nodes, including simple paths that do not visit the same node twice, shortest paths that visit the fewest number of edges, or – as previously discussed in Section&nbsp;<?php echo ref("sssec:dls"); ?> – regular path queries that restrict the labels of edges that can be traversed by the path according to a regular expression&nbsp;<?php echo $references->cite("AnglesABHRV17"); ?>. We could use such algorithms to find, for example, the shortest path(s) in Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> from <span class="gnode">Torres del Paine</span> to <span class="gnode">Moon Valley</span>.</li>
		</ul>
		<p>Most of the aforementioned techniques for graph analytics were originally proposed and studied for simple graphs or directed graphs without edge labels. We will discuss their application to more complex graph models – and how they can be combined with other techniques such as reasoning and querying – later in Section&nbsp;<?php echo ref("sssec:query-languages"); ?>.</p>

		<h4 id="sssec-technologies-graph-analytics" class="subsection">Frameworks</h4>
		<p>Various frameworks have been proposed for large-scale graph analytics, often in a distributed (cluster) setting. Amongst these we can mention Apache Spark (GraphX)&nbsp;<?php echo $references->cite("XinGFS13,DaveJLXGZ16"); ?>, GraphLab&nbsp;<?php echo $references->cite("LowGKBGH12"); ?>, Pregel&nbsp;<?php echo $references->cite("MalewiczABDHLC10"); ?>, Signal–Collect&nbsp;<?php echo $references->cite("signalcollect"); ?>, Shark&nbsp;<?php echo $references->cite("XinRZFSS13"); ?>, etc. These <em>graph parallel frameworks</em> apply a <em>systolic abstraction</em>&nbsp;<?php echo $references->cite("Kung82"); ?> based on a directed graph, where nodes are seen as processors that can send messages to other nodes along edges. Computation is then iterative, where in each iteration, each node reads messages received through inward edges (and possibly its own previous state), performs a computation, and then sends messages through outward edges based on the result. These frameworks then define the systolic computational abstraction on top of the data graph being processed: nodes and edges in the data graph become nodes and edges in the systolic graph.</p>
		<p>To take an example, assume we wish to compute the places that are most (or least) easily reached by the routes shown in the graph of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>. A good way to measure this is using centrality, where we choose PageRank&nbsp;<?php echo $references->cite("page1999pagerank"); ?>, which computes the probability of a tourist randomly following the routes shown in the graph being at a particular place after a given number of “hops”. We can implement PageRank on large graphs using a graph parallel framework. In Figure&nbsp;<?php echo ref("fig:pagerank"); ?>, we provide an example of an iteration of PageRank for an illustrative sub-graph of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>. The nodes are initialised with a score of \(\frac{1}{|V|} = \frac{1}{6}\), where we assume the tourist to have an equal chance of starting at any point. In the <em>message phase</em> (<span class="sc">Msg</span>), each node \(v\) passes a score of \(\frac{d \textrm{R}_i(v)}{|E(v)|}\) on each of its outgoing edges, where we denote by \(d\) a constant damping factor used to ensure convergence (typically \(d = 0.85\), indicating the probability that a tourist randomly “jumps” to any place), by \(\textrm{R}_i(v)\) the score of node \(v\) in iteration \(i\) (the probability of the tourist being at node \(v\) after \(i\) hops), and by \(|E(v)|\) the number of outgoing edges of \(v\). The aggregation phase (<span class="sc">Agg</span>) for \(v\) then sums all incoming messages received along with its constant share of the damping factor (\(\frac{1-d}{|V|}\)) to compute \(\textrm{R}_{i+1}(v)\). We then proceed to the message phase of the next iteration, continuing until some termination criterion is reached (e.g., iteration count or residual threshold, etc.) and final scores are output.</p>

		<figure id="fig-pagerank">
			<img src="images/fig-pagerank.svg" alt="Example of a systolic iteration of PageRank for a sample sub-graph of Figure&nbsp;24"/>
			<figcaption>Example of a systolic iteration of PageRank on a sub-graph of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_1_2_Frameworks/figure_5_3.ttl"></a></figcaption>
		</figure>

		<p>While the given example is for PageRank, the systolic abstraction is general enough to support a wide variety of graph analytics, including those previously mentioned. An algorithm in this framework consists of the functions to compute message values in the <em>message phase</em> (<span class="sc">Msg</span>), and to accumulate the messages in the aggregation phase (<span class="sc">Agg</span>). The framework will take care of distribution, message passing, fault tolerance, etc. However, such frameworks – based on message passing between neighbours – have limitations: not all types of analytics can be expressed in such frameworks&nbsp;<?php echo $references->cite("XuHLJ19"); ?>.<?php echo footnote("Formally, ". $references->citet("XuHLJ19") ." have shown that such frameworks are as powerful as the (incomplete) Weisfeiler–Lehman (WL) graph isomorphism test for distinguishing graphs. This test involves nodes recursively hashing together hashes of local information received from neighbours, and passing these hashes to neighbours."); ?> Hence frameworks may allow additional features, such as a <em>global step</em> that performs a global computation on all nodes, making the result available to each node&nbsp;<?php echo $references->cite("MalewiczABDHLC10"); ?>; or a <em>mutation step</em> that allows for adding or removing nodes and edges during processing&nbsp;<?php echo $references->cite("MalewiczABDHLC10"); ?>.</p>

		<div class="formal">
			<p>Before defining a graph parallel framework, in the interest of generality, we first define a directed graph labelled with feature vectors, which captures the type of input that such a framework can accept, with vectors on both nodes and edges.<p>

			<dl class="definition" id="def-dvlg">
				<dt>Directed vector-labelled graph</dt>
				<dd>We define a <em>directed vector-labelled graph</em> \(G = (V,E,F,\lambda)\), where \(V\) is a set of nodes, \(E \subseteq V \times V\) is a set of edges, \(F\) is a set of feature vectors, and \(\lambda : V \cup E \rightarrow F\) labels each node and edge with a feature vector.</dd>
			</dl>

			<p>A directed-edge labelled graph or property graph may be encoded as a directed vector-labelled graph in a number of ways. The type of node and/or a selection of its attributes may be encoded in the node feature vectors, while the label of an edge and/or a selection of its attributes may be encoded in the edge feature vector (including, for example, weights applied to edges). Typically node feature vectors will all have the same dimensionality, as will edge feature vectors.</p>

			<div class="example" id="ex-dvlg">
				<p>We define a directed vector-labelled graph in preparation for later computing PageRank using a graph parallel framework. Let \(G = (V,E,L)\) denote a directed edge-labelled graph. Let \(|E(u)|\) denote the outdegree of node \(u \in V\). We then initialise a directed vector-labelled graph \(G' = (V,E',F,\lambda)\) such that \(E' = \{ (x,z) \mid \exists y : (x,y,z)\in E \}\), and for all \(u \in V\), we define \(\lambda(u) \coloneqq \begin{bmatrix} \frac{1}{|V|} \\ |E'(u)| \\ |V| \end{bmatrix}\), and \(\lambda(u,v) \coloneqq \begin{bmatrix} \, \end{bmatrix}\), with \(F \coloneqq \{ \lambda(u) \mid u \in V \} \cup \{\lambda(u,v) \mid (u,v) \in E' \}\), assigning each node a vector containing its initial PageRank score, the outdegree of the node, and the number of nodes in the graph. Conversely, edge-vectors are not used in this case.</p>
			</div>

			<p>We now define a graph parallel framework, where we use \(\{\!\!\{ \cdot \}\!\!\}\) to denote a multiset, \(2^{S \rightarrow \mathbb{N}}\) to denote the set of all multisets containing (only) elements from the set \(S\), and \(\mathbb{R}^a\) to denote the set of all vectors of dimension \(a\) (i.e., the set of all vectors containing \(a\) real-valued elements).</p>

			<dl class="definition" id="def-gpf">
				<dt>Graph parallel framework</dt>
				<dd>A <em>graph parallel framework</em> (<em>GPF</em>) is a triple of functions \(\mathfrak{G} \coloneqq (\)<span class="sc">Msg</span>, <span class="sc">Agg</span>, <span class="sc">End</span>\()\) such that (with \(a, b, c \in \mathbb{N}\)):
					<ul>
						<li><span class="sc">Msg</span>\(: \mathbb{R}^a \times \mathbb{R}^b \rightarrow \mathbb{R}^c\)</li>
						<li><span class="sc">Agg</span>\(: \mathbb{R}^a \times 2^{\mathbb{R}^c \rightarrow \mathbb{N}} \rightarrow \mathbb{R}^a\)</li>
						<li><span class="sc">End</span>\(: 2^{\mathbb{R}^a \rightarrow \mathbb{N}} \rightarrow \{ \mathrm{true}, \mathrm{false} \}\)</li>
					</ul>
				</dd>
			</dl>

			<p>The function <span class="sc">Msg</span> defines what message (i.e., vector) must be passed from a node to a neighbouring node along a particular edge, given the current feature vectors of the node and the edge; the function <span class="sc">Agg</span> is used to compute a new feature vector for a node, given its previous feature vector and incoming messages; the function <span class="sc">End</span> defines a condition for termination of vector computation. The integers \(a\), \(b\) and \(c\) denote the dimensions of node feature vectors, edge feature vectors, and message vectors, respectively; we assume that \(a\) and \(b\) correspond with the dimensions of input feature vectors for nodes and edges. Given a GPF \(\mathfrak{G} = (\)<span class="sc">Msg</span>, <span class="sc">Agg</span>, <span class="sc">End</span>\()\), a directed vector-labelled graph \(G = (V, E, F, \lambda)\), and a node \(u \in V\), we define the output vector assigned to node \(u\) in \(G\) by \(\mathfrak{G}\) (written \(\mathfrak{G}(G, u)\)) as follows. First let \(\mathbf{n}_u^{(0)} \coloneqq \lambda(u)\). For all \(i\geq 1\), let:</p>
			<p>\begin{align*}
			 M_u^{(i)} & \coloneqq \left\{\!\!\!\left\{ {\rm\small M{\scriptsize SG}}\left(\mathbf{n}_v^{(i-1)},\lambda(v,u)\right) \bigl\lvert\, (v,u) \in E \right\}\!\!\!\right\} \\
			 \mathbf{n}_{u}^{(i)} & \coloneqq {\rm\small A{\scriptsize GG}}\left(\mathbf{n}_u^{(i-1)},M_u^{(i)}\right)
			\end{align*}</p>
			<p>where \(M_u^{(i)}\) is the multiset of messages received by node \(u\) during iteration \(i\), and \(\mathbf{n}_{u}^{(i)}\) is the state (vector) of node \(u\) at the end of iteration \(i\). If \(j\) is the smallest integer for which <span class="sc">End</span>\((\{\!\!\{ \mathbf{n}_u^{(j)} \mid u \in V \}\!\!\})\) is true, then \(\mathfrak{G}(G, u) \coloneqq \mathbf{n}_u^{(j)}\).</p>
			<p>This particular definition assumes that vectors are dynamically computed for nodes, and that messages are passed only to outgoing neighbours, but the definitions can be readily adapted to consider dynamic vectors for edges, or messages being passed to incoming neighbours, etc. We now provide an example instantiating a GPF to compute PageRank over a directed graph.</p>

			<div class="example">
				<p>We take as input the directed vector labelled graph \(G' = (V,E,F,\lambda)\) from Example&nbsp;<?php echo ref("ex:dvlg"); ?> for a PageRank GPF. First we define the messages passed from \(u\) to \(v\):</p>
				<p class="mathblock"><span class="sc">Msg</span>\(\left(\mathbf{n}_v,\lambda(v,u)\right) \coloneqq \begin{bmatrix}
				\frac{d(\mathbf{n}_{v})_1}{(\mathbf{n}_{v})_2}\\
				\end{bmatrix}\)</p>
				<p>where \(d\) denotes PageRank’s constant dampening factor (typically \(d \coloneqq 0.85\)) and \((\mathbf{n}_{v})_k\) denotes the \(k\)<sup>th</sup> element of the \(\mathbf{n}_{v}\) vector. In other words, \(v\) will pass to \(u\) its PageRank score multiplied by the dampening factor and divided by its out-degree (we do not require \(\lambda(v,u)\) in this particular example). Next we define the function for \(u\) to aggregate the messages it receives from other nodes:</p>
				<p class="mathblock"><span class="sc">Agg</span>\(\left(\mathbf{n}_u,M_u\right) \coloneqq \begin{bmatrix} \frac{1 - d}{(\mathbf{n}_{u})_3} + \sum_{\mathbf{m} \in M_u}(\mathbf{m})_1 \\ (\mathbf{n}_{u})_2 \\ (\mathbf{n}_{u})_3 \\
				\end{bmatrix}\)</p>
				<p>Here, we sum the scores received from other nodes along with its share of rank from the dampening factor, copying over the node’s degree and the total number of nodes for future use. Finally, there are a number of ways that we could define the termination condition; here we simply define:</p>
				<p class="mathblock"><span class="sc">End</span>\((\{\!\!\{ \mathbf{n}_u^{(i)} \mid u \in V \}\!\!\}) \coloneqq (i \geq \textsf{z}) \)</p>
				<p>where \(\textsf{z}\) is a fixed number of iterations, at which point the process stops.</p>
			</div>

			<p>We may note in this example that the total number of nodes is duplicated in the vector for each node of the graph. Part of the benefit of GPFs is that only local information in the neighbourhood of the node is required for each computation step. In practice, such frameworks may allow additional features, such as global computation steps whose results are made available to all nodes&nbsp;<?php echo $references->cite("MalewiczABDHLC10"); ?>, operations that dynamically modify the graph&nbsp;<?php echo $references->cite("MalewiczABDHLC10"); ?>, etc.</p>
		</div>

		<h4 id="sssec-query-languages" class="subsection">Analytics on data graphs</h4>
		<p>As aforementioned, most analytics presented thus far are, in their “native” form, applicable for undirected or directed graphs without the <em>edge metadata</em> – i.e., edge labels or property–value pairs – typical of graph data models.<?php echo footnote("We remark that in the case of property graphs, property–value pairs on nodes can be converted by mapping values to nodes and properties to edges with the corresponding label."); ?> A number of strategies can be applied to make data graphs subject to analytics of this form:</p>
		<ul>
			<li><em>Projection</em> involves simply “projecting” an undirected or directed graph by optionally selecting a sub-graph from the data graph from which all edge meta-data are dropped; for example, the graph of Figure&nbsp;<?php echo ref("fig:pagerank"); ?> may be the result of extracting the sub-graph induced by the edge labels <span class="gelab">bus</span> and <span class="gelab">flight</span> from a larger data graph, where the labels are then dropped to create a directed graph.</li>
			<li><em>Weighting</em> involves converting edge meta-data into numerical values according to some function. Many of the aforementioned techniques are easily adapted to the case of weighted (directed) graphs; for example, we could consider weights on the graph of Figure&nbsp;<?php echo ref("fig:pagerank"); ?> denoting trip duration (or price, traffic, etc.), and then compute the shortest (weighted) paths considering time by adding the duration of each leg of the respective journey.<?php echo footnote("Other forms of analytics are possible if we assume the graph is weighted; for example, if we annotated the graph of Figure&nbsp;". ref("fig:pagerank") ." with probabilities of tourists moving from one place to the next, we could leverage <em>Markov processes</em> to understand features such as reducibility, periodicity, transience, recurrence, ergodicity, steady states, etc., of the routes&nbsp;". $references->cite("markov") ."."); ?> In the absence of external weights, we may rather map edge labels to weights, assigning the same weight to all <span class="gelab">flight</span> edges, to all <span class="gelab">bus</span> edges, etc., based on some criteria.</li>
			<li><em>Transformation</em> involves transforming the graph to a lower arity model. A transformation may be <em>lossy</em>, meaning that the original graph cannot be recovered; or <em>lossless</em>, meaning that the original graph can be recovered. Figure&nbsp;<?php echo ref("fig:transform"); ?> provides an example of a lossy and lossless transformation from a directed edge-labelled graph to directed graphs. In the lossy transformation, we cannot tell, for example, if the original graph contained the edge <?php echo gedge("Iquique","flight","Santiago"); ?>, or rather the edge <?php echo gedge("Iquique","flight","Arica"); ?>, or both. The lossless transformation must introduce new nodes (similar to reification) to maintain information about directed labelled edges. Both transformed graphs further attempt to preserve the directionality of the original graph.</li>
			<li><em>Customisation</em> involves changing the analytical procedure to incorporate edge meta-data, such as was the case for path finding based on path expressions. Other examples might include structural measures for node similarity that not only consider common neighbours, but also common neighbours connected by edges with the same label, or aggregate centrality measures that capture the importance of edges grouped by label, etc.</li>
		</ul>

		<figure id="fig-transform">
			<figure id="fig-transform1" style="display:inline-block;margin-right:2.5em;margin-left:0;">
				<img src="images/fig-transform1.svg" alt="Original graph"/>
				<figcaption>Original graph <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_1_3_Analytics_on_data_graphs/figure_5_4_a.ttl"></a></figcaption>
			</figure>
			<figure id="fig-transform2" style="display:inline-block;">
				<img src="images/fig-transform2.svg" alt="Lossy transformation"/>
				<figcaption>Lossy transformation <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_1_3_Analytics_on_data_graphs/figure_5_4_b.ttl"></a></figcaption>
			</figure>
			<figure id="fig-transform3" style="display:inline-block;margin-right:0;margin-left:2em;">
				<img src="images/fig-transform3.svg" alt="Lossless transformation"/>
				<figcaption>Lossless transformation <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_1_3_Analytics_on_data_graphs/figure_5_4_c.ttl"></a></figcaption>
			</figure>
			<figcaption>Transformations from a directed edge-labelled graph to a directed graph</figcaption>
		</figure>

		<p>The results of an analytical process may change drastically depending on which of the previous strategies are chosen to prepare the graph for analysis. The choice of strategy may be a non-trivial one to make <em>a priori</em> and may require empirical validation. More study is required to more generally understand the effects of such strategies on the results of different analytical techniques over different graph models.</p>

		<h4 id="sssec-analyticsQ" class="subsection">Analytics with queries</h4>
		<p>As discussed in Section&nbsp;<?php echo ref("ssec:querying"); ?>, various languages for querying graphs have been proposed down through the years&nbsp;<?php echo $references->cite("AnglesABHRV17"); ?>. One may consider a variety of ways in which query languages and analytics can complement each other. First, we may consider using query languages to project or transform a graph suitable for a particular analytical task, such as to extract the graph of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> from a larger data graph. Query languages such as SPARQL&nbsp;<?php echo $references->cite("sparql11"); ?>, Cypher&nbsp;<?php echo $references->cite("FrancisGGLLMPRS18"); ?>, and G-CORE&nbsp;<?php echo $references->cite("AnglesABBFGLPPS18"); ?> allow for outputting graphs, where such queries can be used to select sub-graphs for analysis. These languages can also express some limited (non-recursive) analytics, where aggregations can be used to compute degree centrality, for example; they may also have some built-in analytical support, where, for example, Cypher&nbsp;<?php echo $references->cite("FrancisGGLLMPRS18"); ?> allows for finding shortest paths. In the other direction, analytics can contribute to the querying process in terms of <em>optimisations</em>, where, for example, analysis of connectivity may suggest how to better distribute a large data graph over multiple machines for querying using, e.g., <em>minimum cuts</em>&nbsp;<?php echo $references->cite("AkhterNS18,JankeST18"); ?>. Analytics have also been used to <em>rank</em> query results over large graphs&nbsp;<?php echo $references->cite("WagnerTLHS12,FanWW13"); ?>, selecting the most important results for presentation to the user.</p>
		<p>In some use-cases we may further wish to interleave querying and analytical processes. For example, from the full data graph collected by the tourist board, consider an upcoming airline strike where the board wishes to find <em>the events during the strike with venues in cities unreachable from Santiago by public transport due to the strike</em>. Hypothetically, we could use a query to extract the transport network excluding the airline’s routes (assuming, per Figure&nbsp;<?php echo ref("fig:fsa"); ?> that the airline information is available), use analytics to extract the strongly connected component containing Santiago, and finally use a query to find events in cities not in the Santiago component on the given dates.<?php echo footnote("Such a task could not be solved in a single query using regular path queries as such expressions would not be capable of filtering edges representing flights of a particular airline."); ?> While one could solve this task using an imperative language such as Gremlin&nbsp;<?php echo $references->cite("Rodriguez15"); ?>, GraphX&nbsp;<?php echo $references->cite("XinGFS13"); ?>, or R&nbsp;<?php echo $references->cite("R"); ?>, more declarative languages are also being explored to express such tasks, with proposals including the extension of graph query languages with recursive capabilities&nbsp;<?php echo $references->cite("BishofDKLP12,ReutterSV15,HoganRS20"); ?>,<?php echo footnote("Recursive query languages become Turing complete assuming one can also express operations on binary arrays."); ?> combining linear algebra with relational (query) algebra&nbsp;<?php echo $references->cite("HutchisonHS17"); ?>, and so forth.</p>

		<h4 id="sssec-analyticsE" class="subsection">Analytics with entailment</h4>
		<p>Knowledge graphs are often associated with a semantic schema or ontology that defines the semantics of domain terms, giving rise to entailments (per Chapter&nbsp;<?php echo ref("chap:deductive"); ?>). Applying analytics with or without such entailments – e.g., before or after materialisation – may yield radically different results. For example, observe that an edge <?php echo gedge("Santa&nbsp;Lucía","hosts","EID15"); ?> is semantically equivalent to an edge <?php echo gedge("EID15","venue","Santa&nbsp;Lucía"); ?> once the inverse axiom <?php echo gedge("hosts","inv.&nbsp;of","venue"); ?> is invoked; however, these edges are far from equivalent from the perspective of analytical techniques that consider edge direction, for which including one type of edge, or the other, or both, may have a major bearing on the final results. To the best of our knowledge, the combination of analytics and entailment has not been well-explored, leaving open interesting research questions. Along these lines, it may be of interest to explore <em>semantically-invariant analytics</em> that yield the same results over semantically-equivalent graphs (i.e., graphs that entail one another), thus analysing the semantic content of the knowledge graph rather than simply the topological features of the data graph; for example, semantically-invariant analytics would yield the same results over a graph containing the inverse axiom <?php echo gedge("hosts","inv.&nbsp;of","venue"); ?> and a number of <span class="gelab">hosts</span> edges, the same graph but where every <span class="gelab">hosts</span> edge is replaced by an inverse <span class="gelab">venue</span> edge, and the union of both graphs.</p>
		</section>

		<section id="ssec-embeddings" class="section">
		<h3>Knowledge Graph Embeddings</h3>
		<p>Methods for machine learning have gained significant attention in recent years. In the context of knowledge graphs, machine learning can either be used for directly <em>refining</em> a knowledge graph&nbsp;<?php echo $references->cite("Paulheim17"); ?> (discussed further in Chapter&nbsp;<?php echo ref("chap:refine"); ?>); or for <em>downstream tasks</em> using the knowledge graph, such as recommendation&nbsp;<?php echo $references->cite("zhang2016collaborative"); ?>, information extraction&nbsp;<?php echo $references->cite("VashishthJT18"); ?>, question answering&nbsp;<?php echo $references->cite("HuangZLL19"); ?>, query relaxation&nbsp;<?php echo $references->cite("WangWLCZQ18"); ?>, query approximation&nbsp;<?php echo $references->cite("HamiltonBZJL18"); ?>, etc. (discussed further in Chapter&nbsp;<?php echo ref("chap:kgs"); ?>). However, many traditional machine learning techniques assume dense numeric input representations in the form of vectors, which is quite distinct from how graphs are usually expressed. So how can graphs – or nodes, edges, etc., thereof – be encoded as numeric vectors?</p>
		<p>A first attempt to represent a graph using vectors would be to use a <em>one-hot encoding</em>, generating a vector for each node of length \(|L| \cdot |V|\) – with \(|V|\) the number of nodes in the input graph and \(|L|\) the number of edge labels – placing a one at the corresponding index to indicate the existence of the respective edge in the graph, or zero otherwise. Such a representation will, however, typically result in large and sparse vectors, which will be detrimental for most machine learning models.</p>
		<p>The main goal of knowledge graph embedding techniques is to create a dense representation of the graph (i.e., <em>embed</em> the graph) in a continuous, low-dimensional vector space that can then be used for machine learning tasks. The dimensionality \(d\) of the embedding is fixed and usually low (often, e.g., \(50 \geq d \geq 1000\)). Typically the graph embedding is composed of an <em>entity embedding</em> for each node: a vector with \(d\) dimensions that we denote by \(\mathbf{e}\); and a <em>relation embedding</em> for each edge label: (typically) a vector with \(d\) dimensions that we denote by \(\mathbf{r}\). The overall goal of these vectors is to abstract and preserve latent structures in the graph. There are many ways in which this notion of an embedding can be instantiated. Most commonly, given an edge <?php echo gedge("s","p","o"); ?>, a specific embedding approach defines a <em>scoring function</em> that accepts \(\mathbf{e}\)<sub><code>s</code></sub> (the entity embedding of node <span class="gnode">s</span>), \(\mathbf{r}\)<sub><span class="gelab">p</span></sub> (the entity embedding of edge label <span class="gelab">p</span>) and \(\mathbf{e}\)<sub><code>o</code></sub> (the entity embedding of node <span class="gnode">o</span>) and computes the <em>plausibility</em> of the edge, which estimates how likely it is to be true. Given a data graph, the goal is then to compute the embeddings of dimension \(d\) that maximise the plausibility of positive edges (typically edges in the graph) and minimise the plausibility of negative examples (typically edges in the graph with a node or edge label changed such that they are no longer in the graph) according to the given scoring function. The resulting embeddings can then be seen as models learnt through self-supervision that encode (latent) features of the graph, mapping input edges to output plausibility scores.</p>
		<p>Embeddings can then be used for a number of low-level tasks involving the nodes and edge-labels of the graph from which they were computed. First, we can use the plausibility scoring function to assign a confidence to edges that may, for example, have been extracted from an external source (discussed later in Chapter&nbsp;<?php echo ref("chap:create"); ?>). Second, the plausibility scoring function can be used to complete edges with missing nodes/edge labels for the purposes of link prediction (discussed later in Chapter&nbsp;<?php echo ref("chap:refine"); ?>); for example, in Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>, we might ask which nodes in the graph are likely to complete the edge <?php echo gedge("Grey&nbsp;Glacier","bus","?"); ?>, where – aside from <span class="gnode">Punta Arenas</span>, which is already given – we might intuitively expect <span class="gnode">Torres del Paine</span> to be a plausible candidate. Third, embedding models will typically assign similar vectors to similar nodes and similar edge-labels, and thus they can be used as the basis of similarity measures, which may be useful for finding duplicate nodes that refer to the same entity, or for the purposes of providing recommendations (discussed later in Chapter&nbsp;<?php echo ref("chap:kgs"); ?>).</p>
		<p>A wide range of knowledge graph embedding techniques have been proposed&nbsp;<?php echo $references->cite("Wang2017KGEmbedding"); ?>. Our goal here is to provide a high-level introduction to some of the most popular techniques proposed thus far. We first discuss <em>tensor-based approaches</em> that include three different sub-approaches using linear/tensor algebra to compute embeddings. We then discuss <em>language models</em> that leverage existing word embedding techniques, proposing ways of generating graph-like analogues for their expected (textual) inputs. Finally we discuss <em>entailment-aware models</em> that can take into account the semantics of the graph, when available.</p>

		<h4 id="ssec-tensor-based-models" class="subsection">Tensor-based models</h4>
		<p>We first discuss tensor-based models, which we sub-divide into three categories: <em>translational models</em> that adopt a geometric perspective whereby relation embeddings translate subject entities to object entities, <em>tensor decomposition models</em> that extract latent factors approximating the graph’s structure, and <em>neural models</em> that use neural networks to train embeddings that provide accurate plausibility scores.</p>

		<h5 id="sssec-translational-models" class="subsubsection">Translational models</h5>
		<p><em>Translational models</em> interpret edge labels as transformations from subject nodes (aka the <em>source</em> or <em>head</em>) to object nodes (aka the <em>target</em> or <em>tail</em>); for example, in the edge <?php echo gedge("San&nbsp;Pedro","bus","Moon&nbsp;Valley"); ?>, the edge label <span class="gelab">bus</span> is seen as transforming <span class="gnode">San Pedro</span> to <span class="gnode">Moon Valley</span>, and likewise for other <span class="gelab">bus</span> edges. The most elementary approach in this family is TransE&nbsp;<?php echo $references->cite("bordes2013translating"); ?>. Over all positive edges <?php echo gedge("s","p","o"); ?>, TransE learns vectors \(\mathbf{e}\)<sub><code>s</code></sub>, \(\mathbf{r}\)<sub><span class="gelab">p</span></sub>, and \(\mathbf{e}\)<sub><code>os</code></sub> aiming to make \(\mathbf{e}\)<sub><code>s</code></sub>&nbsp;+&nbsp;\(\mathbf{r}\)<sub><span class="gelab">p</span></sub> as close as possible to \(\mathbf{e}\)<sub><code>o</code></sub>. Conversely, if the edge is a negative example, TransE attempts to learn a representation that keeps <span style="white-space:nowrap;">\(\mathbf{e}\)<sub><code>s</code></sub>&nbsp;+&nbsp;\(\mathbf{r}\)<sub><span class="gelab">p</span></sub></span> away from \(\mathbf{e}\)<sub><code>o</code></sub>. To illustrate, Figure&nbsp;<?php echo ref("fig:TransE"); ?> provides a toy example of two-dimensional (\(d = 2\)) entity and relation embeddings computed by TransE. We keep the orientation of the vectors similar to the original graph for clarity. For any edge <?php echo gedge("s","p","o"); ?> in the original graph, adding the vectors <span style="white-space:nowrap;">\(\mathbf{e}\)<sub><code>s</code></sub>&nbsp;+&nbsp;\(\mathbf{r}\)<sub><span class="gelab">p</span></sub></span> should approximate \(\mathbf{e}\)<sub><code>o</code></sub>. In this toy example, the vectors correspond precisely where, for instance, adding the vectors for <span class="gnode">Licantén</span> (\(\mathbf{e}\)<sub><code>L.</code></sub>) and <span class="gelab">west of</span> (\(\mathbf{r}\)<sub><span class="gelab">wo.</span></sub>) gives a vector corresponding to <span class="gnode">Curico</span> (\(\mathbf{e}\)<sub><code>C.</code></sub>). We can use these embeddings to predict edges (amongst other tasks); for example, in order to predict which node in the graph is most likely to be <span class="gelab">west of</span> <span class="gnode">Antofagasta</span> (<code>A.</code>), by computing <span style="white-space:nowrap;">\(\mathbf{e}\)<sub><code>A.</code></sub>&nbsp;+&nbsp;\(\mathbf{r}\)<sub><span class="gelab">wo.</span></sub></span> we find that the resulting vector (dotted in Figure&nbsp;<?php echo ref("fig:transEE"); ?>) is closest to \(\mathbf{e}\)<sub><code>T.</code></sub>, thus predicting <span class="gnode">Toconao</span> (<code>T.</code>) to be the most <em>plausible</em> such node.</p>

		<figure id="fig-TransE">
			<figure id="fig-distEg" style="display:inline-block;margin-right:2.5em;margin-left:0;">
				<img src="images/fig-distEg.svg" alt="Original graph"/>
				<figcaption>Original graph <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_2_1_Tensor-based_models/figure_5_5_a.ttl"></a></figcaption>
			</figure>
			<figure id="fig-transER" style="display:inline-block;">
				<img src="images/fig-transER.svg" alt="Relation embeddings"/>
				<figcaption>Relation embeddings</figcaption>
			</figure>
			<figure id="fig-transEE" style="display:inline-block;margin-right:0;margin-left:2em;">
				<img src="images/fig-transEE.svg" alt="Entity embeddings"/>
				<figcaption>Entity embeddings</figcaption>
			</figure>
			<figcaption>Toy example of two-dimensional relation and entity embeddings learnt by TransE; the entity embeddings use abbreviations and include an example of vector addition to predict what is west of Antofagasta</figcaption>
		</figure>

		<p>Aside from this toy example, TransE can be too simplistic; for example, in Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>, <span class="gelab">bus</span> not only transforms <span class="gnode">San Pedro</span> to <span class="gnode">Moon Valley</span>, but also to <span class="gnode">Arica</span>, <span class="gnode">Calama</span>, and so forth. TransE will, in this case, aim to give similar vectors to all such target locations, which may not be feasible given other edges. TransE will also tend to assign cyclical relations a zero vector, as the directional components will tend to cancel each other out. To resolve such issues, many variants of TransE have been investigated. Amongst these, for example, TransH&nbsp;<?php echo $references->cite("wang2014knowledge"); ?> represents different relations using distinct hyperplanes, where for the edge <?php echo gedge("s","p","o"); ?>, <span class="gnode">s</span> is first projected onto the hyperplane of <span class="gelab">p</span> before the translation to <span class="gnode">o</span> is learnt (uninfluenced by edges with other labels for <span class="gnode">s</span> and for <span class="gnode">o</span>). TransR&nbsp;<?php echo $references->cite("lin2015learning"); ?> generalises this approach by projecting <span class="gnode">s</span> and <span class="gnode">o</span> into a vector space specific to <span class="gelab">p</span>, which involves multiplying the entity embeddings for <span class="gnode">s</span> and <span class="gnode">o</span> by a projection matrix specific to <span class="gelab">p</span>. TransD&nbsp;<?php echo $references->cite("TransD"); ?> simplifies TransR by associating entities and relations with a second vector, where these secondary vectors are used to project the entity into a relation-specific vector space. Recently, RotatE&nbsp;<?php echo $references->cite("SunDNT19"); ?> proposes translational embeddings in complex space, which allows to capture more characteristics of relations, such as direction, symmetry, inversion, antisymmetry, and composition. Embeddings have also been proposed in non-Euclidean space; for example, MuRP&nbsp;<?php echo $references->cite("BalazevicAH19"); ?> uses relation embeddings that transform entity embeddings in the hyperbolic space of the Poincaré ball mode, whose curvature provides more “space” to separate entities with respect to the dimensionality. For discussion of other translational models, we refer to surveys by <?php echo $references->citet("CaiZC18"); ?>, <?php echo $references->citet("Wang2017KGEmbedding"); ?>.</p>

		<h5 id="sssec-tensor-decomposition-models" class="subsubsection">Tensor decomposition models</h5>
		<p>A second approach to derive graph embeddings is to apply methods based on <em>tensor decomposition</em>. A <em>tensor</em> is a multidimensional numeric field that generalises scalars (\(0\)-order tensors), vectors (\(1\)-order tensors) and matrices (\(2\)-order tensors) towards arbitrary dimension/order. Tensors have become a widely used abstraction for machine learning&nbsp;<?php echo $references->cite("RabanserSG17"); ?>. Tensor decomposition involves decomposing a tensor into more “elemental” tensors (e.g., of lower order) from which the original tensor can be recomposed (or approximated) by a fixed sequence of basic operations over the output tensors. These elemental tensors can be viewed as capturing <em>latent factors</em> underlying the information contained in the original tensor. There are many approaches to tensor decomposition, where we will now briefly introduce the main ideas behind <em>rank decompositions</em>&nbsp;<?php echo $references->cite("RabanserSG17"); ?>.</p>
		<p>Leaving aside graphs momentarily, consider an \((a,b)\)-matrix (i.e., a \(2\)-order tensor) \(\mathbf{C}\), where \(a\) is the number of cities in Chile, \(b\) is the number of months in a year, and each element \((\mathbf{C})_{ij}\) denotes the average temperature of the \(i\)<sup>th</sup> city in the \(j\)<sup>th</sup> month. Noting that Chile is a long, thin country – ranging from subpolar climates in the south, to a desert climate in the north – we may find a decomposition of \(\mathbf{C}\) into two vectors representing latent factors – specifically \(\mathbf{x}\) (with \(a\) elements) giving lower values for cities with lower latitude, and \(\mathbf{y}\) (with \(b\) elements), giving lower values for months with lower temperatures – such that computing the outer product<?php echo footnote("The outer product of two (column) vectors \\(\\mathbf{x}\\) of length \\(a\\) and \\(\\mathbf{y}\\) of length \\(b\\), denoted \\(\\mathbf{x} \\otimes \\mathbf{y}\\), is defined as \\(\\mathbf{x}\\mathbf{y}^{\\mathrm{T}}\\), yielding an \\((a,b)\\)-matrix \\(\\mathbf{M}\\) such that \\((\\mathbf{M})_{ij} = (\\mathbf{x})_i \\cdot (\\mathbf{y})_j\\). Analogously, the outer product of \\(k\\) vectors is a \\(k\\)-order tensor."); ?> of the two vectors approximates \(\mathbf{C}\) reasonably well: \(\mathbf{x} \otimes \mathbf{y} \approx \mathbf{C}\). In the (unlikely) case that there exist vectors \(\mathbf{x}\) and \(\mathbf{y}\) such that \(\mathbf{C}\) is precisely the outer product of two vectors (\(\mathbf{x} \otimes \mathbf{y} = \mathbf{C}\)) we call \(\mathbf{C}\) a rank-\(1\) matrix; we can then precisely encode \(\mathbf{C}\) using \(a + b\) values rather than \(a \times b\) values. Most times, however, to get precisely \(\mathbf{C}\), we need to sum multiple rank-\(1\) matrices, where the rank \(r\) of \(\mathbf{C}\) is the minimum number of rank-\(1\) matrices that need to be summed to derive precisely \(\mathbf{C}\), such that \(\mathbf{x}_1 \otimes \mathbf{y}_1 + \ldots \mathbf{x}_r \otimes \mathbf{y}_r = \mathbf{C}\). In the temperature example, \(\mathbf{x}_2 \otimes \mathbf{y}_2\) might correspond to a correction for altitude, \(\mathbf{x}_3 \otimes \mathbf{y}_3\) for higher temperature variance further south, etc. A (low) rank decomposition of a matrix then sets a limit \(d\) on the rank and computes the vectors \((\mathbf{x}_1,\mathbf{y}_1,\ldots,\mathbf{x}_{d},\mathbf{y}_{d})\) such that \(\mathbf{x}_1 \otimes \mathbf{y}_1 + \ldots + \mathbf{x}_{d} \otimes \mathbf{y}_{d}\) gives the best \(d\)-rank approximation of \(\mathbf{C}\). Noting that to generate \(n\)-order tensors we need to compute the outer product of \(n\) vectors, we can generalise this idea towards low-rank decomposition of tensors; this method is called Canonical Polyadic (CP) decomposition&nbsp;<?php echo $references->cite("Hitchcock27"); ?>. For example, a \(3\)-order tensor \(\mathcal{C}\) containing monthly temperatures for Chilean cities <em>at four different times of day</em> could be approximated with \(\mathbf{x}_1 \otimes \mathbf{y}_1 \otimes \mathbf{z}_1 + \ldots \mathbf{x}_{d} \otimes \mathbf{y}_{d} \otimes \mathbf{z}_{d}\) (e.g., \(\mathbf{x}_1\) might be a latitude factor, \(\mathbf{y}_1\) a monthly variation factor, and \(\mathbf{z}_1\) a daily variation factor, and so on). Various algorithms exist to compute (approximate) CP decompositions, including Alternating Least Squares, Jennrich’s Algorithm, and the Tensor Power method&nbsp;<?php echo $references->cite("RabanserSG17"); ?>.</p>
		<p>Returning to graphs, similar principles can be used to decompose a graph into vectors, thus yielding embeddings. In particular, a graph can be encoded as a one-hot \(3\)-order tensor \(\mathcal{G}\) with \(|V| \times |L| \times |V|\) elements, where the element \((\mathcal{G})_{ijk}\) is set to one if the \(i\)<sup>th</sup> node links to the \(k\)<sup>th</sup> node with an edge having the \(j\)<sup>th</sup> label, or zero otherwise. As previously mentioned, such a tensor will typically be very large and sparse, where rank decompositions are thus applicable. A CP decomposition&nbsp;<?php echo $references->cite("Hitchcock27"); ?> would compute a sequence of vectors \((\mathbf{x}_1,\mathbf{y}_1,\mathbf{z}_1,\ldots,\mathbf{x}_d,\mathbf{y}_d,\mathbf{z}_d)\) such that \(\mathbf{x}_1 \otimes \mathbf{y}_1 \otimes \mathbf{z}_1 + \ldots + \mathbf{x}_d \otimes \mathbf{y}_d \otimes \mathbf{z}_d \approx \mathcal{G}\). We illustrate this scheme in Figure&nbsp;<?php echo ref("fig:cpRank"); ?>. Letting \(\mathbf{X}, \mathbf{Y}, \mathbf{Z}\) denote the matrices formed by \(\begin{bmatrix} \mathbf{x}_1\,\cdots\,\mathbf{x}_d \end{bmatrix}\), \(\begin{bmatrix} \mathbf{y}_1\,\cdots\,\mathbf{y}_d \end{bmatrix}\), \(\begin{bmatrix} \mathbf{z}_1\,\cdots\,\mathbf{z}_d \end{bmatrix}\), respectively, with each vector forming a column of the corresponding matrix, we could then extract the \(i\)<sup>th</sup> row of \(\mathbf{Y}\) as an embedding for the \(i\)<sup>th</sup> relation, and the \(j\)<sup>th</sup> rows of \(\mathbf{X}\) and \(\mathbf{Z}\) as <em>two</em> embeddings for the \(j\)<sup>th</sup> entity. However, knowledge graph embeddings typically aim to assign <em>one</em> vector to each entity.</p>

		<figure id="fig-cpRank">
			<img src="images/fig-cpRank.svg" alt="Abstract illustration of a CP \(d\)-rank decomposition of a tensor representing the graph of Figure&nbsp;27a"/>
			<figcaption>Abstract illustration of a CP \(d\)-rank decomposition of a tensor representing the graph of Figure&nbsp;<?php echo ref("fig:distEg"); ?></figcaption>
		</figure>

		<p>DistMult&nbsp;<?php echo $references->cite("distmult"); ?> is a seminal method for computing knowledge graph embeddings based on rank decompositions, where each entity and relation is associated with a vector of dimension \(d\), such that for an edge <?php echo gedge("s","p","o"); ?>, a plausibility scoring function \(\sum_{i=1}^d (\mathbf{e}\)<sub><code>s</code></sub>\()_i (\mathbf{r}\)<sub><span class="gelab">p</span></sub>\()_i (\mathbf{e}\)<sub><code>o</code></sub>\()_i\) is defined, where <span style="white-space:nowrap;">\((\mathbf{e}\)<sub><code>s</code></sub>\()_i\)</span>, <span class="nobreak">\((\mathbf{r}\)<sub><span class="gelab">p</span></sub>\()_i\)</span> and \((\mathbf{e}\)<sub><code>o</code></sub>\()_i\) denote the \(i\)<sup>th</sup> elements of vectors \(\mathbf{e}\)<sub><code>s</code></sub>, \(\mathbf{r}\)<sub><span class="gelab">p</span></sub>, \(\mathbf{e}\)<sub><code>o</code></sub>, respectively. The goal, then, is to learn vectors for each node and edge label that maximise the plausibility of positive edges and minimise the plausibility of negative edges. This approach equates to a CP decomposition of the graph tensor \(\mathcal{G}\), but where entities have one vector that is used twice: \(\mathbf{x}_1 \otimes \mathbf{y}_1 \otimes \mathbf{x}_1 + \ldots + \mathbf{x}_d \otimes \mathbf{y}_d \otimes \mathbf{x}_d \approx \mathcal{G}\). A weakness of this approach is that per the scoring function, the plausibility of <?php echo gedge("s","p","o"); ?> will always be equal to that of <?php echo gedge("o","p","s"); ?>; in other words, DistMult does not consider edge direction.</p>
		<p>Rather than use a vector as a relation embedding, RESCAL&nbsp;<?php echo $references->cite("nickel2013tensor"); ?> uses a matrix, which allows for combining values from \(\mathbf{e}\)<sub><code>s</code></sub> and \(\mathbf{e}\)<sub><code>o</code></sub> across all dimensions, and thus can capture (e.g.) edge direction. However, RESCAL incurs a higher cost in terms of space and time than DistMult. HolE&nbsp;<?php echo $references->cite("NickelRP16"); ?> uses vectors for relation and entity embeddings, but proposes to use the <em>circular correlation operator</em> – which takes sums along the diagonals of the outer product of two vectors – to combine them. This operator is not commutative, and can thus consider edge direction. ComplEx&nbsp;<?php echo $references->cite("TrouillonWRGB16"); ?>, on the other hand, uses a complex vector (i.e., a vector containing complex numbers) as a relational embedding, which similarly allows for breaking the aforementioned symmetry of DistMult’s scoring function while keeping the number of parameters low. SimplE&nbsp;<?php echo $references->cite("Kazemi018"); ?> rather proposes to compute a standard CP decomposition computing two initial vectors for entities from \(\mathbf{X}\) and \(\mathbf{Z}\) and then averaging terms across \(\mathbf{X}\), \(\mathbf{Y}\), \(\mathbf{Z}\) to compute the final plausibility scores. TuckER&nbsp;<?php echo $references->cite("BalazevicAH19a"); ?> employs a different type of decomposition – called a Tucker Decomposition&nbsp;<?php echo $references->cite("tucker64extension"); ?>, which computes a smaller “core” tensor \(\mathcal{T}\) and a sequence of three matrices \(\mathbf{A}\), \(\mathbf{B}\) and \(\mathbf{C}\), such that \(\mathcal{G} \approx \mathcal{T} \otimes \mathbf{A} \otimes \mathbf{B} \otimes \mathbf{C}\) – where entity embeddings are taken from \(\mathbf{A}\) and \(\mathbf{C}\), while relation embeddings are taken from \(\mathbf{B}\). Of these approaches, TuckER&nbsp;<?php echo $references->cite("BalazevicAH19a"); ?> currently provides state-of-the-art results on standard benchmarks.</p>

		<h5 id="sssec-neural-models" class="subsubsection">Neural models</h5>
		<p>A limitation of the aforementioned approaches is that they assume either linear (preserving addition and scalar multiplication) or bilinear (e.g., matrix multiplication) operations over embeddings to compute plausibility scores. Other approaches rather use neural networks to learn embeddings with non-linear scoring functions for plausibility.</p>
		<p>One of the earliest proposals of a neural model was Semantic Matching Energy (SME)&nbsp;<?php echo $references->cite("GlorotBWB13"); ?>, which learns parameters (aka weights: \(\mathbf{w}\), \(\mathbf{w}'\)) for two functions – \(f_{\mathbf{w}}(\mathbf{e}\)<sub><code>s</code></sub>\(,\mathbf{r}\)<sub><span class="gelab">p</span></sub>\()\) and \(g_{\mathbf{w}'}(\mathbf{e}\)<sub><code>o</code></sub>\(,\mathbf{r}\)<sub><span class="gelab">p</span></sub>\()\) – such that the dot product of the result of both functions – \(f_{\mathbf{w}}(\mathbf{e}\)<sub><code>s</code></sub>\(,\mathbf{r}\)<sub><span class="gelab">p</span></sub>\() \cdot g_{\mathbf{w}'}(\mathbf{e}\)<sub><code>o</code></sub>\(,\mathbf{r}\)<sub><span class="gelab">p</span></sub>\()\) – gives the plausibility score. Both linear and bilinear variants of \(f_{\mathbf{w}}\) and \(g_{\mathbf{w}'}\) are proposed. Another early proposal was Neural Tensor Networks (NTN)&nbsp;<?php echo $references->cite("socher2013reasoning"); ?>, which proposes to maintain a tensor \(\mathcal{W}\) of internal weights, such that the plausibility score is computed by a complex function that combines the outer product <span class="nobreak">\(\mathbf{e}\)<sub><code>s</code></sub>\( \otimes \mathcal{W} \otimes \mathbf{e}\)<sub><code>o</code></sub></span> with a standard neural layer over \(\mathbf{e}\)<sub><code>s</code></sub> and \(\mathbf{e}\)<sub><code>o</code></sub>, which in turn is combined with \(\mathbf{r}\)<sub><span class="gelab">p</span></sub>, to produce a plausibility score. The tensor \(\mathcal{W}\) results in a high number of parameters, limiting scalability&nbsp;<?php echo $references->cite("Wang2017KGEmbedding"); ?>. Multi-Layer Perceptron (MLP)&nbsp;<?php echo $references->cite("DongGHHLMSSZ14"); ?> is a simpler model, where \(\mathbf{e}\)<sub><code>s</code></sub>, \(\mathbf{r}\)<sub><span class="gelab">p</span></sub> and \(\mathbf{e}\)<sub><code>o</code></sub> are concatenated and fed into a hidden layer to compute plausibility scores.</p>
		<p>A number of more recent approaches have proposed using convolutional kernels in their models. ConvE&nbsp;<?php echo $references->cite("DettmersMS018"); ?> proposes to generate a matrix from \(\mathbf{e}\)<sub><code>s</code></sub> and \(\mathbf{r}\)<sub><span class="gelab">p</span></sub> by “wrapping” each vector over several rows and concatenating both matrices. The concatenated matrix serves as the input for a set of (2D) convolutional layers, which returns a feature map tensor. The feature map tensor is vectorised and projected into \(d\) dimensions using a parameterised linear transformation. The plausibility score is then computed based on the dot product of this vector and \(\mathbf{e}\)<sub><code>o</code></sub>. A disadvantage of ConvE is that by wrapping vectors into matrices, it imposes an artificial two-dimensional structure on the embeddings. HypER&nbsp;<?php echo $references->cite("BalazevicAH19b"); ?> is a similar model using convolutions, but avoids the need to wrap vectors into matrices. Instead, a fully connected layer (called the “hypernetwork”) is applied to \(\mathbf{r}\)<sub><span class="gelab">p</span></sub> and used to generate a matrix of relation-specific convolutional filters. These filters are applied directly to \(\mathbf{e}\)<sub><code>s</code></sub> to give a feature map, which is vectorised. The same process is then applied as in ConvE: the resulting vector is projected into \(d\) dimensions, and a dot product applied with \(\mathbf{e}\)<sub><code>o</code></sub> to produce the plausibility score. The resulting model is shown to outperform ConvE on standard benchmarks&nbsp;<?php echo $references->cite("BalazevicAH19b"); ?>.</p>
		<p>The presented approaches strike different balances in terms of expressivity and the number of parameters than need to be trained. While more expressive models, such as NTN, may better fit more complex plausibility functions over lower dimensional embeddings by using more hidden parameters, simpler models, such as that proposed by Dong et al.&nbsp;<?php echo $references->cite("DongGHHLMSSZ14"); ?>, and convolutional networks&nbsp;<?php echo $references->cite("DettmersMS018,BalazevicAH19b"); ?> that enable parameter sharing by applying the same (typically small) kernels over different regions of a matrix, require handling fewer parameters overall and are more scalable.</p>

		<h5 id="sssec-survey-and-def" class="subsubsection">Survey and definition of tensor-based approaches</h5>
		<p>We now formally define and survey the aforementioned tensor-based approaches. For simplicity, we will consider directed edge-labelled graphs.</p>

		<div class="formal">
			<p>Before defining embeddings, we first introduce tensors.</p>

			<dl class="definition" id="def-vector-matrix-tensor-order-mode">
				<dt>Vector, matrix, tensor, order, mode</dt>
				<dd>For any positive integer \(a\), a <em>vector</em> of dimension \(a\) is a family of real numbers indexed by integers in \(\{1, \ldots, a\}\). For \(a\) and \(b\) positive integers, an \((a,b)\)-matrix is a family of real numbers indexed by pairs of integers in \(\{1, \ldots, a\} \times \{1, \ldots, b\}\). A tensor is a family of real numbers indexed by a finite sequence of integers such that there exist positive numbers \(a_1, \ldots, a_n\) such that the indices are all the tuples of numbers in \(\{1, \ldots, a_1\} \times \ldots \times \{1, \ldots, a_n\}\). The number \(n\) is called the <em>order</em> of the tensor, the subindices \(i\in \{1, \ldots, n\}\) indicate the <em>mode</em> of a tensor, and each \(a_i\) defines the dimension of the \(i\)<sup>th</sup> mode. A 1-order tensor is a vector and a 2-order tensor is a matrix. We denote the set of all tensors as \(\mathbb{T}\).</dd>
			</dl>

			<p>For specific dimensions \(a_1,\ldots,a_n\) of modes, a tensor is an element of \((\cdots(\mathbb{R}^{a_1})^{\ldots})^{a_n}\) but we write \(\mathbb{R}^{a_1,\ldots,a_n}\) to simplify the notation. We use lower-case bold font to denote vectors (\(\mathbf{x} \in \mathbb{R}^a\)), upper-case bold font to denote matrices (\(\mathbf{X} \in \mathbb{R}^{a,b}\)) and calligraphic font to denote tensors <span style="white-space:nowrap;">(\(\mathcal{X} \in \mathbb{R}^{a_1,\ldots,a_n}\)).</span></p>
			<p>Now we are ready to abstractly define knowledge graph embeddings.</p>

			<dl class="definition" id="def-knowledge-graph-embedding">
				<dt>Knowledge graph embedding</dt>
				<dd>Given a directed edge-labelled graph \(G = (V,E,L)\), a <em>knowledge graph embedding of \(G\)</em> is a pair of mappings \((\varepsilon,\rho)\) such that \(\varepsilon : V \rightarrow \mathbb{T}\) and \(\rho : L \rightarrow \mathbb{T}\).</dd>
			</dl>

			<p>In the most typical case, \(\varepsilon\) and \(\rho\) map nodes and edge-labels, respectively, to vectors of fixed dimension. In some cases, however, they may map to matrices. Given this abstract notion of a knowledge graph embedding, we can then define a plausibility scoring function.</p>

			<dl class="definition" id="def-plausibility">
				<dt>Plausibility scores</dt>
				<dd>A <em>plausibility scoring function</em> is a partial function \(\phi : \mathbb{T} \times \mathbb{T} \times \mathbb{T} \rightarrow \mathbb{R}\). Given a directed edge-labelled graph \(G = (V,E,L)\), an edge \((s,p,o) \in V \times L \times V\), and a knowledge graph embedding \((\varepsilon,\rho)\) of \(G\), the plausibility of \((s,p,o)\) is given as \(\phi(\varepsilon(s),\rho(p),\varepsilon(o))\).</dd>
			</dl>

			<p>Edges with higher scores are considered more plausible. Given a graph \(G = (V,E,L)\), we assume a set of positive edges \(E^+\) and a set of negative edges \(E^{-}\). Positive edges are often simply the edges in the graph: \(E^+ \coloneqq E\). Negative edges use the vocabulary of \(G\) (i.e., \(E^- \subseteq V \times L \times V\)) and are typically defined by taking edges \((s,p,o)\) from \(E\) and changing one term of each edge – often one of the nodes – such that the edge is no longer in \(E\). Given sets of positive and negative edges, and a plausibility scoring function, the objective is then to find the embedding that maximises the plausibility of edges in \(E^+\) while minimising the plausibility of edges in \(E^{-}\). Specific knowledge graph embeddings then instantiate the type of embedding considered and the plausibility scoring function in various ways.</p>
			<p>In Table&nbsp;<?php echo ref("tab:kges"); ?>, we define the plausibility scoring function and types of embeddings used by different knowledge graph embeddings. To simplify the definitions, we use \(\mathbf{e}_x\) to denote \(\varepsilon(x)\) when it is a vector, \(\mathbf{r}_y\) to denote \(\rho(y)\) when it is a vector, and \(\mathbf{R}_y\) to denote \(\rho(y)\) when it is a matrix. Some models involve learnt parameters (aka weights) for computing plausibility. We denote these as <span style="white-space:nowrap;">\(\mathbf{v}\), \(\mathbf{V}\), \(\mathcal{V}\), \(\mathbf{w}\), \(\mathbf{W}\), \(\mathcal{W}\)</span> (for vectors, matrices or tensors). We use \(d_e\) and \(d_r\) to denote the dimensionality chosen for entity embeddings and relation embeddings, respectively. Often it is assumed that \(d_e = d_r\), in which case we will write \(d\). Weights may have their own dimensionality, which we denote \(w\). The embeddings in Table&nbsp;<?php echo ref("tab:kges"); ?> use a variety of operators on vectors, matrices and tensors, which will be defined later.</p>

			<p>The embeddings defined in Table&nbsp;<?php echo ref("tab:kges"); ?> vary in complexity, where a trade-off exists between the number of parameters used, and the expressiveness of the model in terms of its capability to capture latent features of the graph. To increase expressivity, many of the models in Table&nbsp;<?php echo ref("tab:kges"); ?> use additional parameters beyond the embeddings themselves. A possible formal guarantee of such models is <em>full expressiveness</em>, which, given any disjoint sets of positive edges \(E^+\) and negative edges \(E^{-}\), asserts that the model can always correctly partition those edges. On the one hand, for example, DistMult&nbsp;<?php echo $references->cite("distmult"); ?> cannot distinguish an edge <?php echo gedge("s","p","o"); ?> from its inverse <?php echo gedge("o","p","s"); ?>, so by adding an inverse of an edge in \(E^+\) to \(E^{-}\), we can show that it is <em>not</em> fully expressive. On the other hand, models such as ComplEx&nbsp;<?php echo $references->cite("TrouillonWRGB16"); ?>, SimplE&nbsp;<?php echo $references->cite("Kazemi018"); ?>, and TuckER&nbsp;<?php echo $references->cite("BalazevicAH19a"); ?> have been proven to be fully expressive given sufficient dimensionality; for example, TuckER&nbsp;<?php echo $references->cite("BalazevicAH19a"); ?> with dimensions \(d_r = |L|\) and \(d_e = |V|\) trivially satisfies full expressivity since its core tensor \(\mathcal{W}\) then has sufficient capacity to store the full one-hot encoding of any graph. This formal property is useful to show that the model does not have built-in limitations for numerically representing a graph, though of course in practice the dimensions needed to reach full expressivity are often impractical/undesirable.</p>
			<p>We continue by first defining the conventions used in Table&nbsp;<?php echo ref("tab:kges"); ?>.</p>
			<ul>
				<li>We use \((\mathbf{x})_{i}\), \((\mathbf{X})_{ij}\), and \((\mathcal{X})_{{i_1}\ldots{i_n}}\) to denote elements of vectors, matrices, and tensors, respectively. If a vector \(\mathbf{x} \in \mathbb{R}^a\) is used in a context that requires a matrix, the vector is interpreted as an \((a, 1)\)-matrix (i.e., a column vector) and can be turned into a row vector (i.e., a \((1,a)\)-matrix) using the transpose operation \(\mathbf{x}^T\). We use \(\mathbf{x}^\mathrm{D} \in \mathbb{R}^{a,a}\) to denote the diagonal matrix with the values of the vector \(\mathbf{x} \in \mathbb{R}^{a}\) on its diagonal. We denote the identity matrix by \(\mathbf{I}\) such that if \(j=k\), then \((\mathbf{I})_{jk} = 1\); otherwise \((\mathbf{I})_{jk} = 0\).</li>
				<li>We denote by \(\begin{bmatrix}\mathbf{X}_1\\[-0.5ex]\vdots\\\mathbf{X_n}\end{bmatrix}\) the vertical stacking of matrices \(\mathbf{X}_1, \ldots, \mathbf{X}_n\) with the same number of columns. Given a vector \(\mathbf{x} \in \mathbb{R}^{ab}\), we denote by \(\mathbf{x}^{[a,b]} \in \mathbb{R}^{a,b}\) the “reshaping” of \(\mathbf{x}\) into an \((a,b)\)-matrix such that \((\mathbf{x}^{[a,b]})_{ij} = (\mathbf{x})_{(i + a(j-1))}\). Conversely, given a matrix \(\mathbf{X} \in \mathbb{R}^{a,b}\), we denote by \(\mathrm{vec}(\mathbf{X}) \in \mathbb{R}^{ab}\) the <em>vectorisation</em> of \(\mathbf{X}\) such that \(\mathrm{vec}(\mathbf{X})_k = (\mathbf{X})_{ij}\) where \(i = ((k-1)\,\mathrm{mod}\,m) + 1\) and \(j = \frac{k - i}{m} + 1\) (observe that \(\mathrm{vec}(\mathbf{x}^{[a,b]}) = \mathbf{x}\)).</li>
				<li>Given a tensor \(\mathcal{X} \in \mathbb{R}^{a,b,c}\), we denote by \(\mathcal{X}^{[i:\cdot:\cdot]} \in \mathbb{R}^{b,c}\), the \(i\)<sup>th</sup> <em>slice</em> of tensor \(\mathcal{X}\) along the first mode; for example, given \(\mathcal{X} \in \mathbb{R}^{5,2,3}\), then \(\mathcal{X}^{[4:\cdot:\cdot]}\) returns the \((2,3)\)-matrix consisting of the elements \(\begin{bmatrix} (\mathcal{X})_{411} & (\mathcal{X})_{412} & (\mathcal{X})_{413} \\ (\mathcal{X})_{421} & (\mathcal{X})_{422} & (\mathcal{X})_{423} \end{bmatrix}\). Analogously, we use \(\mathcal{X}^{[\cdot : i : \cdot]} \in \mathbb{R}^{a,c}\) and \(\mathcal{X}^{[\cdot:\cdot:i]} \in \mathbb{R}^{b,c}\) to indicate the \(i\)<sup>th</sup> slice along the second and third modes of \(\mathcal{X}\), respectively.</li>
				<li>We denote by \(\psi(\mathcal{X})\) the element-wise application of a function \(\psi\) to the tensor \(\mathcal{X}\), such that \((\psi(\mathcal{X}))_{in_1\ldots i_n} = \psi(\mathcal{X}_{i_1\ldots i_n})\). Common choices for \(\psi\) include a sigmoid function (e.g., the logistic function \(\psi(x) = \frac{1}{1 + e^{-x}}\) or the hyperbolic tangent function \(\psi(x) = \mathrm{tanh}\,x = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)), the rectifier (\(\psi(x) = \mathrm{max}(0,x)\)), softplus (\(\psi(x) = \mathrm{ln}(1 + e^x)\)), etc.</li>
			</ul>

			<p>We now define the operators used in Table&nbsp;<?php echo ref("tab:kges"); ?>, where the first and most elemental operation we consider is that of matrix multiplication.</p>

			<dl class="definition" id="def-matrix-multiplication">
				<dt>Matrix multiplication</dt>
				<dd>The <em>multiplication of matrices</em> \(\mathbf{X} \in \mathbb{R}^{a,b}\) and \(\mathbf{Y} \in \mathbb{R}^{b,c}\) is a matrix \(\mathbf{XY} \in \mathbb{R}^{a,c}\) such that \((\mathbf{XY})_{ij} = \sum_{k=1}^b (\mathbf{X})_{ik}(\mathbf{Y})_{kj}\). The matrix multiplication of two tensors \(\mathcal{X} \in \mathbb{R}^{a_1,\ldots,a_m,c}\) and \(\mathcal{Y} \in \mathbb{R}^{c,b_1,\ldots,b_n}\) is a tensor \(\mathcal{XY} \in \mathbb{R}^{a_1,\ldots,a_{m},b_{1},\ldots,b_{n}}\) such that (\(\mathcal{XY})_{i_1\ldots i_m i_{m+1}\ldots i_{m+n}} = \sum_{k=1}^c (\mathcal{X})_{i_1\ldots i_m k}(\mathcal{Y})_{k i_{m+1}i_{m+n}}\).</dd>
			</dl>

			<p>For convenience, we may implicitly add or remove modes with dimension 1 for the purposes of matrix multiplication and other operators; for example, given two vectors \(\mathbf{x} \in \mathbb{R}^{a}\) and \(\mathbf{y} \in \mathbb{R}^{a}\), we denote by \(\T{\mathbf{x}}\mathbf{y}\) (aka the dot or inner product) the multiplication of matrix \(\T{\mathbf{x}} \in \mathbb{R}^{1,a}\) with \(\mathbf{y} \in \mathbb{R}^{a,1}\) such that \(\T{\mathbf{x}}\mathbf{y} \in \mathbb{R}^{1,1}\) (i.e., a scalar in \(\mathbb{R}\)); conversely, \(\mathbf{x}\T{\mathbf{y}} \in \mathbb{R}^{a,a}\) (the outer product).</p>
			<p>Constraints on embeddings are sometimes given as norms, defined next.</p>

			<dl class="definition" id="def-lpnorm-lpqnorm">
				<dt>\(L^p\)-norm, \(L^{p,q}\)-norm</dt>
				<dd>For \(p\in \mathbb{R}\), the <em>\(L^p\)-norm</em> of a vector \(\mathbf{x}\in \mathbb{R}^a\) is the scalar \(\|\mathbf{x}\|_p \coloneqq (|(\mathbf{x})_1|^p + \ldots + |(\mathbf{x})_a|^p)^{\frac{1}{p}}\), where \(|(\mathbf{x})_i|\) denotes the absolute value of the \(i\)<sup>th</sup> element of \(\mathbf{x}\). For \(p,q\in \mathbb{R}\), the <em>\(L^{p,q}\)-norm</em> of a matrix \(\mathbf{X}\in\mathbb{R}^{a,b}\) is the scalar \(\|\mathbf{X}\|_{p,q} \coloneqq \left( \sum_{j=1}^b \left( \sum_{i=1}^a |(\mathbf{X})_{ij}|^p \right)^{\frac{q}{p}} \right)^\frac{1}{q}\).</dd>
			</dl>

			<p>The \(L^1\) norm (i.e., \(\|\mathbf{x}\|_1\)) is thus simply the sum of the absolute values of \(\mathbf{x}\), while the \(L^2\) norm (i.e., \(\|\mathbf{x}\|_2\)) is the (Euclidean) length of the vector. The Frobenius norm of the matrix \(\mathbf{X}\) then equates to \(\|\mathbf{X}\|_{2,2} = \left( \sum_{j=1}^b \left( \sum_{i=1}^a |(\mathbf{X})_{ij}|^2 \right) \right)^\frac{1}{2}\); i.e., the square root of the sum of the squares of all elements.</p>
			<p>Another type of product used by embedding techniques is the Hadamard product, which multiplies tensors of the same dimension and computes their product in an element-wise manner.</p>

			<dl class="definition" id="def-hadamard-product">
				<dt>Hadamard product</dt>
				<dd>Given two tensors \(\mathcal{X} \in \mathbb{R}^{a_1,\ldots,a_n}\) and \(\mathcal{Y} \in \mathbb{R}^{a_1,\ldots,a_n}\), the <em>Hadamard product</em> \(\mathcal{X} \odot \mathcal{Y}\) is defined as a tensor in \(\mathbb{R}^{a_1,\ldots,a_n}\), with each element computed as \((\mathcal{X} \odot \mathcal{Y})_{i_1\ldots i_{n}} \coloneqq (\mathcal{X})_{i_1\ldots i_{n}} (\mathcal{Y})_{i_1\ldots i_{n}}\).</dd>
			</dl>

			<p>Other embedding techniques – namely RotatE&nbsp;<?php echo $references->cite("SunDNT19"); ?> and ComplEx&nbsp;<?php echo $references->cite("TrouillonWRGB16"); ?> – uses <em>complex space</em> based on complex numbers. With a slight abuse of notation, the definitions of vectors, matrices and tensors can be modified by replacing the set of real numbers \(\mathbb{R}\) by the set of complex numbers \(\mathbb{C}\), giving rise to complex vectors, complex matrices, and complex tensors. In this case, we denote by \(\mathrm{Re}(\cdot)\) the real part of a complex number. Given a complex vector \(\mathbf{x} \in \mathbb{C}^I\), we denote by \(\overline{\mathbf{x}}\) its complex conjugate (swapping the sign of the imaginary part of each element). Complex analogues of the aforementioned operators can then be defined by replacing the multiplication and addition of real numbers with the analogous operators for complex numbers, where RotateE&nbsp;<?php echo $references->cite("SunDNT19"); ?> uses the complex Hadamard product, and ComplEx&nbsp;<?php echo $references->cite("TrouillonWRGB16"); ?> uses complex matrix multiplication.</p>
			<p>One embedding technique – MuRP&nbsp;<?php echo $references->cite("BalazevicAH19"); ?> – uses hyperbolic space, specifically based on the Poincaré ball. As this is the only embedding we cover that uses this space, and the formalisms are lengthy (covering the Poincaré ball, Möbius addition, Möbius matrix–vector multiplication, logarithmic maps, exponential maps, etc.), we rather refer the reader to the paper for further details&nbsp;<?php echo $references->cite("BalazevicAH19"); ?>.</p>
			<p>As discussed in Section&nbsp;<?php echo ref("ssec:embeddings"); ?>, tensor decompositions are used for many embeddings, and at the heart of such decompositions is the tensor product, which is often used to reconstruct (an approximation of) the original tensor.</p>

			<dl class="definition" id="def-tensor-product">
				<dt>Tensor product</dt>
				<dd>Given two tensors \(\mathcal{X} \in \mathbb{R}^{a_1,\ldots,a_m}\) and \(\mathcal{Y} \in \mathbb{R}^{b_1,\ldots,b_n}\), the <em>tensor product</em> \(\mathcal{X} \otimes \mathcal{Y}\) is defined as a tensor in \(\mathbb{R}^{a_1,\ldots,a_m,b_1,\ldots,b_n}\), with each element computed as \((\mathcal{X} \otimes \mathcal{Y})_{i_1\ldots i_{m}j_1\ldots j_n} \coloneqq (\mathcal{X})_{i_1 \ldots i_m} (\mathcal{Y})_{j_1 \ldots j_n}\).<?php echo footnote("Please note that “\\(\\otimes\\)” is used here in an unrelated sense to its use in Definition&nbsp;". ref("def:anndom") ."."); ?></dd>
			</dl>

			<div class="example">
				<p>Assume that \(\mathcal{X} \in \mathbb{R}^{2,3}\) and \(\mathcal{Y} \in \mathbb{R}^{3,4,5}\). Then \(\mathcal{X} \otimes \mathcal{Y}\) will be a tensor in \(\mathbb{R}^{2,3,3,4,5}\). Element \((\mathcal{X} \otimes \mathcal{Y})_{12345}\) will be the product of \((\mathcal{X})_{12}\) and \((\mathcal{Y})_{345}\).</p>
			</div>

			<p>An \(n\)-mode product is used by other embeddings to transform elements along a given mode of a tensor by computing a product with a given matrix along that particular mode of the tensor.</p>

			<dl class="definition" id="def-nmode-product">
				<dt>\(n\)-mode product</dt>
				<dd>For a positive integer \(n\), a tensor \(\mathcal{X} \in \mathbb{R}^{a_1,\ldots,a_{n-1},a_n,a_{n+1},\ldots,a_m}\) and matrix \(\mathbf{Y} \in \mathbb{R}^{b,a_n}\), the <em>\(n\)-mode product</em> of \(\mathcal{X}\) and \(\mathbf{Y}\) is the tensor \(\mathcal{X} \otimes_n \mathbf{Y} \in \mathbb{R}^{a_1,\ldots,a_{n-1},b,a_{n+1},\ldots,a_m}\) such that \((\mathcal{X} \otimes_n \mathbf{Y})_{i_1\ldots i_{n-1}ji_{n+1}\ldots i_m} \coloneqq \sum_{k=1}^{a_n} (\mathcal{X})_{i_1 \ldots i_{n-1}ki_{n+1} \ldots i_m} (\mathbf{Y})_{jk}\).</dd>
			</dl>

			<div class="example">
				<p>Let us assume that \(\mathcal{X} \in \mathbb{R}^{2,3,4}\) and \(\mathbf{Y} \in \mathbb{R}^{5,3}\). The result of \(\mathcal{X} \otimes_2 \mathbf{Y}\) will be a tensor in \(\mathbb{R}^{2,5,4}\), where, for example, \((\mathcal{X} \otimes_2 \mathbf{Y})_{142}\) will be given as \((\mathcal{X})_{112}(\mathbf{Y})_{41} + (\mathcal{X})_{122}(\mathbf{Y})_{42} + (\mathcal{X})_{132}(\mathbf{Y})_{43}\). Observe that if \(\mathbf{y} \in \mathbb{R}^{a_n}\) – i.e., if \(\mathbf{y}\) is a (column) vector – then the \(n\)-mode tensor product \(\mathcal{X} \otimes_n \T{\mathbf{y}}\) “flattens” the \(n\)<sup>th</sup> mode of \(\mathcal{X}\) to one dimension, effectively reducing the order of \(\mathcal{X}\) by one.</p>
			</div>

			<p>One embedding technique – HolE&nbsp;<?php echo $references->cite("NickelRP16"); ?> – uses the circular correlation operator \(\mathbf{x} \star \mathbf{y}\), where each element is the sum of elements along a diagonal of the outer product \(\mathbf{x} \otimes \mathbf{y}\) that “wraps” if not the primary diagonal.</p>

			<dl class="definition" id="def-circular-correlation">
				<dt>Circular correlation</dt>
				<dd>The <em>circular correlation</em> of vector \(\mathbf{x} \in \mathbb{R}^a\) with \(\mathbf{y} \in \mathbb{R}^a\) is the vector \(\mathbf{x} \star \mathbf{y} \in \mathbb{R}^{a}\) such that \((\mathbf{x} \star \mathbf{y})_k \coloneqq \sum_{i=1}^a (\mathbf{x})_i (\mathbf{y})_{(((k+i-2) \,\mathrm{mod}\,a)+1)}\). </dd>
			</dl>

			<div class="example">
				<p>Assuming \(a = 5\), then \((\mathbf{x} \star \mathbf{y})_1 = (\mathbf{x})_1(\mathbf{y})_1 + (\mathbf{x})_2(\mathbf{y})_2 + (\mathbf{x})_3(\mathbf{y})_3 + (\mathbf{x})_4(\mathbf{y})_4 + (\mathbf{x})_5(\mathbf{y})_5\), or a case that wraps: \((\mathbf{x} \star \mathbf{y})_4 = (\mathbf{x})_1(\mathbf{y})_4 + (\mathbf{x})_2(\mathbf{y})_5 + (\mathbf{x})_3(\mathbf{y})_1 + (\mathbf{x})_4(\mathbf{y})_2 + (\mathbf{x})_5(\mathbf{y})_3\).</p>
			</div>

			<p>Finally, a couple of neural models that we include – namely ConvE&nbsp;<?php echo $references->cite("DettmersMS018"); ?> and HypER&nbsp;<?php echo $references->cite("BalazevicAH19b"); ?> – are based on convolutional architectures using the convolution operator.</p>

			<dl class="definition" id="def-convolution">
				<dt>Convolution</dt>
				<dd>Given two matrices \(\mathbf{X} \in \mathbb{R}^{a,b}\) and \(\mathbf{Y} \in \mathbb{R}^{e,f}\), the <em>convolution</em> of \(\mathbf{X}\) and \(\mathbf{Y}\) is the matrix \(\mathbf{X} * \mathbf{Y} \in \mathbb{R}^{(a + e - 1),(b + f - 1)}\) such that \((\mathbf{X} * \mathbf{Y})_{ij} = \sum_{k=1}^a \sum_{l=1}^b (\mathbf{X})_{kl} (\mathbf{Y})_{(i+k-a)(j+l-b)}\).<?php echo footnote("We define the convolution operator per the widely-usedconvention for convolutional neural networks. Strictly speaking, the operator should be called <em>cross-correlation</em>, where traditional convolution requires the matrix \\(\\mathbf{X}\\) to be initially “rotated” by 180°. Since in our settings the matrix \\(\\mathbf{X}\\) is learnt, rather than given, the rotation is redundant, and hence the distinction is not important."); ?> In cases where \((i+k-a) &lt; 1\), \((j+l-b) &lt; 1\), \((i+k-a) &gt; e\) or \((j+l-b) &gt; f\) (i.e., where \((\mathbf{Y})_{(i+k-a)(j+l-b)}\) lies outside the bounds of \(\mathbf{Y}\)), we say that \((\mathbf{Y})_{(i+k-a)(j+l-b)} = 0\).</dd>
			</dl>

			<p>Intuitively speaking, the convolution operator overlays \(\mathbf{X}\) in every possible way over \(\mathbf{Y}\) such that at least one pair of elements \((\mathbf{X})_{ij},(\mathbf{Y})_{lk}\) overlaps, summing the products of pairs of overlapping elements to generate an element of the result. Elements of \(\mathbf{X}\) extending beyond \(\mathbf{Y}\) are ignored (equivalently we can consider \(\mathbf{Y}\) to be “zero-padded” outside its borders).</p>
			<div class="example">
				<p>Given \(\mathbf{X} \in \mathbb{R}^{3,3}\) and \(\mathbf{Y} \in \mathbb{R}^{4,5}\), then \(\mathbf{X} * \mathbf{Y} \in \mathbb{R}^{6,7}\), where, for example, \((\mathbf{X} * \mathbf{Y})_{11} = (\mathbf{X})_{33}(\mathbf{Y})_{11}\) (with the bottom right corner of \(\mathbf{X}\) overlapping the top left corner of \(\mathbf{Y}\)), while \((\mathbf{X} * \mathbf{Y})_{34} = (\mathbf{X})_{11}(\mathbf{Y})_{12} + \)\( (\mathbf{X})_{12}(\mathbf{Y})_{13} + \)\( (\mathbf{X})_{13}(\mathbf{Y})_{14} + \)\( (\mathbf{X})_{21}(\mathbf{Y})_{22} + \)\( (\mathbf{X})_{22}(\mathbf{Y})_{23} + \)\( (\mathbf{X})_{23}(\mathbf{Y})_{24} + \)\( (\mathbf{X})_{31}(\mathbf{Y})_{32} + \)\( (\mathbf{X})_{32}(\mathbf{Y})_{33} + \)\( (\mathbf{X})_{33}(\mathbf{Y})_{34}\) (with \((\mathbf{X})_{22}\) – the centre of \(\mathbf{X}\) – overlapping \((\mathbf{Y})_{23}\)).<?php echo footnote("Models applying convolutions may differ regarding how edge cases are handled, or on the “stride” of the convolution applied, where, for example, a stride of 3 for \\((\\mathbf{X} * \\mathbf{Y})\\) would see the kernel \\(\\mathbf{X}\\) centred only on elements \\((\\mathbf{Y})_{ij}\\) such that \\(i\\,\\mathrm{mod}\\,3 = 0\\) and \\(j\\,\\mathrm{mod}\\,3 = 0\\), reducing the number of output elements by a factor of 9. We do not consider such details here."); ?></p>
			</div>

			<p>In a convolution \(\mathbf{X} * \mathbf{Y}\), the matrix \(\mathbf{X}\) is often called the “kernel” (or “filter”). Often several kernels are used in order to apply multiple convolutions. Given a tensor \(\mathcal{X} \in \mathbb{R}^{c,a,b}\) (representing \(c\) \((a,b)\)-kernels) and a matrix \(\mathbf{Y} \in \mathbb{R}^{e,f}\), we denote by \(\mathcal{X} * \mathbf{Y} \in \mathbb{R}^{c,(a + e - 1),(b + f - 1)}\) the result of the convolutions of the \(c\) first-mode slices of \(\mathcal{X}\) over \(\mathbf{Y}\) such that \((\mathcal{X} * \mathbf{Y})^{[i:\cdot:\cdot]} = \mathcal{X}^{[i:\cdot:\cdot]} * \mathbf{Y}\) for \(1 \leq i \leq c\), yielding a tensor of results for \(c\) convolutions.</p>
		</div>

		<div class="formal-table">
			<table id="tab-kges" class="condensedTable">
				<caption>Details for selected knowledge graph embeddings, including the plausibility scoring function \(\phi(\varepsilon(s),\rho(p),\varepsilon(o))\) for edge <?php echo gedge("\(s\)","\(p\)","\(o\)"); ?>, and other conditions</caption>
				<thead>
					<tr>
						<th><strong>Model</strong></th>
						<th>\(\phi(\varepsilon(s),\rho(p),\varepsilon(o))\)</th>
						<th><strong>Conditions</strong> (for all \(x \in V\), \(y \in L\))</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td>TransE</td>
						<td>\(- \|\mathbf{e}_s + \mathbf{r}_p - \mathbf{e}_o\|_q\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(q \in \{1,2\}\), \(\|\mathbf{e}_x\|_2 = 1\)</td>
					</tr>
					<tr>
						<td>TransH</td>
						<td>\(-\|(\mathbf{e}_s - (\T{\mathbf{e}_s}\mathbf{w}_p)\mathbf{w}_p) + \mathbf{r}_p - (\mathbf{e}_o - (\T{\mathbf{e}_o} \mathbf{w}_p)\mathbf{w}_p)\|^{2}_{2}\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\mathbf{w}_y \in \mathbb{R}^d\),<br />
							\(\|\mathbf{w}_y\|_2 = 1\) , \(\frac{\T{\mathbf{w}_y} \mathbf{r}_y}{\|\mathbf{r}_y\|_2} \approx 0\), \(\|\mathbf{e}_x\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>TransR</td>
						<td>\(-\|\mathbf{W}_p\mathbf{e}_s + \mathbf{r}_p - \mathbf{W}_p\mathbf{e}_o\|^{2}_{2}\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d_e}\), \(\mathbf{r}_y \in \mathbb{R}^{d_r}\), \(\mathbf{W}_y \in \mathbb{R}^{d_r , d_e}\),<br />
							\(\|\mathbf{e}_x\|_2 \leq 1\), \(\|\mathbf{r}_y\|_2 \leq 1\), \(\|\mathbf{W}_y\mathbf{e}_x\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>TransD</td>
						<td>\(-\|(\mathbf{w}_p\otimes\mathbf{w}_s + \mathbf{I})\mathbf{e}_s + \mathbf{r}_p - (\mathbf{w}_p\otimes\mathbf{w}_o + \mathbf{I})\mathbf{e}_o\|^{2}_{2}\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d_e}\), \(\mathbf{r}_y \in \mathbb{R}^{d_r}\), \(\mathbf{w}_x \in \mathbb{R}^{d_e}\), \(\mathbf{w}_y \in \mathbb{R}^{d_r}\),<br />
							\(\|\mathbf{e}_x\|_2 \leq 1\), \(\|\mathbf{r}_y\|_2 \leq 1\), \(\|(\mathbf{w}_y\otimes\mathbf{w}_x + \mathbf{I})\mathbf{e}_x\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>RotatE</td>
						<td>\(- \|\mathbf{e}_s \odot \mathbf{r}_p - \mathbf{e}_o\|_2\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{C}^{d}\), \(\mathbf{r}_y \in \mathbb{C}^{d}\), \(\|\mathbf{r}_y\|_2 = 1\)</td>
					</tr>
					<tr>
						<td>RESCAL</td>
						<td>\(\T{\mathbf{e}_s} \mathbf{R}_p \mathbf{e}_o\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{R}_y \in \mathbb{R}^{d,d}\), \(\|\mathbf{e}_x\|_2 \leq 1\), \(\|\mathbf{R}_y\|_{2,2} \leq 1\)</td>
					</tr>
					<tr>
						<td>DistMult</td>
						<td>\(\T{\mathbf{e}_s} \D{\mathbf{r}_p} \mathbf{e}_o\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\|\mathbf{e}_x\|_2 = 1\), \(\|\mathbf{r}_y\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>HolE</td>
						<td>\(\T{\mathbf{r}_p} (\mathbf{e}_s \star \mathbf{e}_o)\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\|\mathbf{e}_x\|_2 \leq 1\), \(\|\mathbf{r}_y\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>ComplEx</td>
						<td>\(\mathrm{Re}(\T{\mathbf{e}_s} \D{\mathbf{r}_p} \overline{\mathbf{e}}_o)\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{C}^{d}\), \(\mathbf{r}_y \in \mathbb{C}^{d}\), \(\|\mathbf{e}_x\|_2 \leq 1\), \(\|\mathbf{r}_y\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>SimplE</td>
						<td>\(\frac{\T{\mathbf{e}_s} \D{\mathbf{r}_p} \mathbf{w}_o + \T{\mathbf{e}_o} \D{\mathbf{w}_p} \mathbf{w}_s}{2}\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\mathbf{w}_x \in \mathbb{R}^{d}\), \(\mathbf{w}_y \in \mathbb{R}^{d}\),<br />
							\(\|\mathbf{e}_x\|_2 \leq 1\), \(\|\mathbf{w}_x\|_2 \leq 1\), \(\|\mathbf{r}_y\|_2 \leq 1, \|\mathbf{w}_y\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>TuckER</td>
						<td>\(\mathcal{W} \otimes_1 \T{\mathbf{e}_s} \otimes_2 \T{\mathbf{r}_p} \otimes_3 \T{\mathbf{e}_o}\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d_e}\), \(\mathbf{r}_y \in \mathbb{R}^{d_r}\), \(\mathcal{W} \in \mathbb{R}^{d_e , d_r , d_e }\)</td>
					</tr>
					<tr>
						<td>SME L.</td>
						<td>\(\T{(\mathbf{V}\mathbf{e}_s + \mathbf{V}'\mathbf{r}_p + \mathbf{v})} (\mathbf{W}\mathbf{e}_o + \mathbf{W}'\mathbf{r}_p + \mathbf{w})\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\mathbf{v} \in \mathbb{R}^w\), \(\mathbf{w} \in \mathbb{R}^w\), \(\|\mathbf{e}_x\|_2 = 1\),<br />
							\(\mathbf{V} \in \mathbb{R}^{w,d},\mathbf{V}' \in \mathbb{R}^{w,d}, \mathbf{W} \in \mathbb{R}^{w,d}, \mathbf{W}' \in \mathbb{R}^{w,d}\)</td>
					</tr>
					<tr>
						<td>SME Bi.</td>
						<td>\(\T{((\mathcal{V} \otimes_3 \T{\mathbf{r}_p}) \mathbf{e}_s + \mathbf{v})}((\mathcal{W} \otimes_3 \T{\mathbf{r}_p}) \mathbf{e}_o + \mathbf{w})\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\mathbf{v} \in \mathbb{R}^w\), \(\mathbf{w} \in \mathbb{R}^w\), \(\|\mathbf{e}_x\|_2 = 1\),<br />
							\(\mathcal{V} \in \mathbb{R}^{w,d,d}\), \(\mathcal{W} \in \mathbb{R}^{w,d,d}\)</td>
					</tr>
					<tr>
						<td>NTN</td>
						<td>\(\T{\mathbf{r}_p} \psi\left(\T{\mathbf{e}_s} \mathcal{W} \mathbf{e}_o + \mathbf{W} \begin{bmatrix}\mathbf{e}_s\\\mathbf{e}_o\end{bmatrix} + \mathbf{w}\right) \)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\mathbf{w} \in \mathbb{R}^{w}\), \(\mathbf{W} \in \mathbb{R}^{w , 2d}\),<br />
							\(\mathcal{W} \in \mathbb{R}^{d,w,d}\), \(\|\mathbf{e}_x\|_2 \leq 1\), \(\|\mathbf{r}_y\|_2 \leq 1\),<br />
							\(\|\mathbf{w}\|_2 \leq 1\), \(\|\mathbf{W}\|_{2,2} \leq 1\), \(\|\mathcal{W}^{[\cdot:i:\cdot]}_{1\leq i \leq w}\|_{2,2} \leq 1\)</td>
					</tr>
					<tr>
						<td>MLP</td>
						<td>\(\T{\mathbf{v}} \psi\left(\mathbf{W} \begin{bmatrix}\mathbf{e}_s\\\mathbf{r}_p\\\mathbf{e}_o\end{bmatrix} + \mathbf{w}\right) \)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(\mathbf{v} \in \mathbb{R}^{w}\), \(\mathbf{w} \in \mathbb{R}^{w}\), \(\mathbf{W} \in \mathbb{R}^{w , 3d}\)<br />
							\(\|\mathbf{e}_x\|_2 \leq 1\) \(\|\mathbf{r}_y\|_2 \leq 1\)</td>
					</tr>
					<tr>
						<td>ConvE</td>
						<td>\(\psi\left(\T{\mathrm{vec}\left(\psi\left( \mathcal{W} * \begin{bmatrix}\mathbf{e}_s^{[a, b]}\\\mathbf{r}_p^{[a, b]}\end{bmatrix} \right)\right)} \mathbf{W}\right) \mathbf{e}_o \)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d}\), \(\mathbf{r}_y \in \mathbb{R}^{d}\), \(d = ab\),<br />
							\(\mathbf{W} \in \mathbb{R}^{w_1(w_2 + 2a - 1)(w_3 + b - 1) , d}\), \(\mathcal{W} \in \mathbb{R}^{w_1 , w_2 , w_3}\)</td>
					</tr>
					<tr>
						<td>HypER</td>
						<td>\(\psi\T{\left(\mathrm{vec}\left( \T{\mathbf{r}_p} \mathcal{W} * \mathbf{e}_s \right)} \mathbf{W} \right) \mathbf{e}_o\)</td>
						<td>\(\mathbf{e}_x \in \mathbb{R}^{d_e}\), \(\mathbf{r}_y \in \mathbb{R}^{d_r}\), \(\mathbf{W} \in \mathbb{R}^{w_2(w_1 + d_e - 1) , d_e}\),<br />
							\(\mathcal{W} \in \mathbb{R}^{d_r , w_1 , w_2}\)</td>
					</tr>
				</tbody>
			</table>
		</div>

		<h4 id="sssec-language-models" class="subsection">Language models</h4>
		<p>Embedding techniques were first explored as a way to represent natural language within machine learning frameworks, with word2vec&nbsp;<?php echo $references->cite("mikolov2013efficient"); ?> and GloVe&nbsp;<?php echo $references->cite("pennington2014glove"); ?> being two seminal approaches. Both approaches compute embeddings for words based on large corpora of text such that words used in similar contexts (e.g., “<code>frog</code>”, “<code>toad</code>”) have similar vectors. Word2vec uses neural networks trained either to predict the current word from surrounding words (<em>continuous bag of words</em>), or to predict the surrounding words given the current word (<em>continuous skip-gram</em>). GloVe rather applies a regression model over a matrix of co-occurrence probabilities of word pairs. Embeddings generated by both approaches have become widely used in natural language processing tasks.</p>
		<p>Another approach for graph embeddings is thus to leverage proven approaches for language embeddings. However, while a graph consists of an unordered set of sequences of three terms (i.e., a set of edges), text in natural language consists of arbitrary-length sequences of terms (i.e., sentences of words). RDF2Vec&nbsp;<?php echo $references->cite("ristoski2016rdf2vec"); ?> thus performs (biased&nbsp;<?php echo $references->cite("cochez2017biased"); ?>) random walks on the graph and records the paths (the sequence of nodes and edge labels traversed) as “sentences”, which are then fed as input into the word2vec&nbsp;<?php echo $references->cite("mikolov2013efficient"); ?> model. An example of such a path extracted from Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> might be, for example, <?php echo gedge("San&nbsp;Pedro","bus","Calama");?><?php echo esource(); ?><span class="edge">flight</span><?php echo etipr(); ?><?php echo gedge("Iquique","flight","Santiago");?>, where the paper experiments with \(500\) paths of length \(8\) per entity. RDF2Vec also proposes a second mode where sequences are generated for nodes from canonically-labelled sub-trees of which they are a root node, where sub-trees of depth \(1\) and \(2\) are used for experiments. KGloVe&nbsp;<?php echo $references->cite("cochez2017global"); ?> is rather based on GloVe. Given that the original GloVe model&nbsp;<?php echo $references->cite("pennington2014glove"); ?> considers words that co-occur frequently in windows of text to be more related, KGloVe uses personalised PageRank<?php echo footnote("Intuitively speaking, personalised PageRank starts at a given node and then determines the probability of a random walk being at a particular node after a given number of steps. A higher number of steps converges towards standard PageRank emphasising global node centrality in the graph, while a lower number emphasises proximity/relatedness to the starting node."); ?> to determine the most related nodes to a given node, which are fed into the GloVe model.</p>

		<h4 id="sssec-entailment-aware-models" class="subsection">Entailment-aware models</h4>
		<p>The embeddings thus far consider the data graph alone. But what if an ontology or set of rules is provided? Such deductive knowledge could be used to improve the embeddings. One approach is to use constraint rules to refine the predictions made by embeddings; for example, <?php echo $references->citet("WangWG15"); ?> use functional and inverse-functional definitions as constraints (under UNA) such that, for example, if we define that an event can have at most one value for <span class="gelab">venue</span>, this is used to lower the plausibility of edges that would assign multiple venues to an event.</p>
		<p>More recent approaches rather propose joint embeddings that consider both the data graph and rules when computing embeddings. KALE&nbsp;<?php echo $references->cite("GuoWWWG16"); ?> computes entity and relation embeddings using a translational model (specifically TransE) that is adapted to further consider rules using <em>t-norm fuzzy logics</em>. With reference to Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>, consider a simple rule <?php echo sedge("?x","bus",NULL,"?y","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?x","connects&nbsp;to",NULL,"?y","gvar"); ?>. We can use embeddings to assign plausibility scores to new edges, such as \(e_1\): <?php echo gedge("Piedras&nbsp;Rojas","bus","Moon&nbsp;Valley"); ?>. We can further apply the previous rule to generate a new edge \(e_2\): <?php echo gedge("Piedras&nbsp;Rojas","connects&nbsp;to","Moon&nbsp;Valley"); ?> from the predicted edge \(e_1\). But what plausibility should we assign to this second edge? Letting \(p_1\) and \(p_2\) be the current plausibility scores of \(e_1\) and \(e_2\) (initialised using the standard embedding), then t-norm fuzzy logics suggests that the plausibility be updated as \(p_1p_2 - p_1 + 1\). Embeddings are then trained to jointly assign larger plausibility scores to positive examples versus negative examples of both edges and <em>ground rules</em>. An example of a positive ground rule based on Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> would be <?php echo gedge("Arica","bus","San&nbsp;Pedro"); ?> \(\Rightarrow\) <?php echo gedge("Arica","connects&nbsp;to","San&nbsp;Pedro"); ?>. Negative ground rules randomly replace the relation in the head of the rule; for example, <?php echo gedge("Arica","bus","San&nbsp;Pedro"); ?> \(\not\Rightarrow\) <?php echo gedge("Arica","flight","San&nbsp;Pedro"); ?>. <?php echo $references->citet("GuoWWWG18"); ?> later propose RUGE, which uses a joint model over ground rules (possibly soft rules with confidence scores) and plausibility scores to align both forms of scoring for unseen edges.</p>
		<p>Generating ground rules can be costly. An alternative approach, called FSL&nbsp;<?php echo $references->cite("DemeesterRR16"); ?>, observes that in the case of a simple rule, such as <?php echo sedge("?x","bus",NULL,"?y","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?x","connects&nbsp;to",NULL,"?y","gvar"); ?>, the relation embedding <span class="gelab">bus</span> should always return a lower plausibility than <span class="gelab">connects to</span>. Thus, for all such rules, FSL proposes to train relation embeddings while avoiding violations of such inequalities. While relatively straightforward, FSL only supports simple rules, while KALE also supports more complex rules.</p>
		<p>These works exemplify how deductive and inductive forms of knowledge – in this case rules and embeddings – can interplay and complement each other.</p>
		</section>

		<section id="ssec-gnns" class="section">
		<h3>Graph Neural Networks</h3>
		<p>While embeddings aim to provide a dense numerical representation of graphs suitable for use within existing machine learning models, another approach is to build custom machine learning models adapted for graph-structured data. Most custom learning models for graphs are based on (artificial) neural networks&nbsp;<?php echo $references->cite("abs-1901-00596"); ?>, exploiting a natural correspondence between both: a neural network already corresponds to a weighted, directed graph, where nodes serve as artificial neurons, and edges serve as weighted connections (axons). However, the typical topology of a traditional neural network – more specifically, a fully-connected feed-forward neural network – is quite homogeneous, being defined in terms of sequential layers of nodes where each node in one layer is connected to all nodes in the next layer. Conversely, the topology of a data graph is quite heterogeneous, being determined by the relations between entities that its edges represent.</p>
		<p>A <em>graph neural network</em> (GNN)&nbsp;<?php echo $references->cite("ScarselliGTHM09"); ?> builds a neural network based on the topology of the data graph; i.e., nodes are connected to their neighbours per the data graph. Typically a model is then learnt to map input features for nodes to output features in a supervised manner; output features of the example nodes used for training may be manually labelled, or may be taken from the knowledge graph. Unlike knowledge graph embeddings, GNNs support end-to-end supervised learning for specific tasks: given a set of labelled examples, GNNs can be used to classify elements of the graph or the graph itself. GNNs have been used to perform classification over graphs encoding compounds, objects in images, documents, etc.; as well as to predict traffic, build recommender systems, verify software, etc.&nbsp;<?php echo $references->cite("abs-1901-00596"); ?>. Given labelled examples, GNNs can even replace graph algorithms; for example, GNNs have been used to find central nodes in knowledge graphs in a supervised manner&nbsp;<?php echo $references->cite("ScarselliGTHM09,ParkKDZF19,ParkKDZF20"); ?>.</p>
		<p>We now discuss the ideas underlying two main flavours of GNN, specifically, <em>recursive GNNs</em> and <em>non-recursive GNNs</em>.</p>

		<h4 id="sssec-recursive-gnn" class="subsection">Recursive graph neural networks</h4>
		<p>Recursive graph neural networks (RecGNNs) are the seminal approach to graph neural networks&nbsp;<?php echo $references->cite("SperdutiS97,ScarselliGTHM09"); ?>. The approach is conceptually similar to the systolic abstraction illustrated in Figure&nbsp;<?php echo ref("fig:pagerank"); ?>, where messages are passed between neighbours towards recursively computing some result. However, rather than define the functions used to decide the messages to pass, we rather label the output of a training set of nodes and let the framework learn the functions that generate the expected output, thereafter applying them to label other examples.</p>
		<p>In a seminal paper, <?php echo $references->citet("ScarselliGTHM09"); ?> proposed what they generically call a graph neural network (GNN), which takes as input a directed graph where nodes and edges are associated with <em>feature vectors</em> that can capture node and edge labels, weights, etc. These feature vectors remain fixed throughout the process. Each node in the graph is also associated with a <em>state vector</em>, which is recursively updated based on information from the node’s neighbours – i.e., the feature and state vectors of the neighbouring nodes and the feature vectors of the edges extending to/from them – using a parametric function, called the <em>transition function</em>. A second parametric function, called the <em>output function</em>, is used to compute the final output for a node based on its own feature and state vector. These functions are applied recursively up to a fixpoint. Both parametric functions can be implemented using neural networks where, given a partial set of <em>supervised nodes</em> in the graph – i.e., nodes labelled with their desired output – parameters for the transition and output functions can be learnt that best approximate the supervised outputs. The result can thus be seen as a recursive neural network architecture.<?php echo footnote("Some authors refer to such architectures as <em>recurrent graph neural networks</em>, observing that the internal state maintained for nodes can be viewed as a form of recurrence over a sequence of transitions."); ?> To ensure convergence up to a fixpoint, certain restrictions are applied, namely that the transition function be a <em>contractor</em>, meaning that upon each application of the function, points in the numeric space are brought closer together (intuitively, in this case, the numeric space “shrinks” upon each application, ensuring convergence to a unique fixpoint).</p>
		<p>To illustrate, consider, for example, that we wish to find priority locations for creating new tourist information offices. A good strategy would be to install them in hubs from which many tourists visit popular destinations. Along these lines, in Figure&nbsp;<?php echo ref("fig:gnn"); ?> we illustrate the GNN architecture proposed by <?php echo $references->citet("ScarselliGTHM09"); ?> for a sub-graph of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?>, where we highlight the neighbourhood of <span class="gnode">Punta Arenas</span>. In this graph, nodes are annotated with feature vectors (\(\mathbf{n}_x\)) and hidden states at step \(t\) (\(\mathbf{h}_x^{(t)}\)), while edges are annotated with feature vectors (\(\mathbf{a}_{xy}\)). Feature vectors for nodes may, for example, one-hot encode the type of node (<em>City</em>, <em>Attraction</em>, etc.), directly encode statistics such as the number of tourists visiting per year, etc. Feature vectors for edges may, for example, one-hot encode the edge label (the type of transport), directly encode statistics such as the distance or number of tickets sold per year, etc. Hidden states can be randomly initialised. The right-hand side of Figure&nbsp;<?php echo ref("fig:gnn"); ?> provides the GNN transition and output functions, where \(\mathrm{N}(x)\) denotes the neighbouring nodes of \(x\), \(f_{\mathbf{w}}(\cdot)\) denotes the transition function with parameters \(\mathbf{w}\), and \(g_{\mathbf{w}'}(\cdot)\) denotes the output function with parameters \(\mathbf{w'}\). An example is also provided for Punta Arenas (\(x = 1\)). These functions will be recursively applied until a fixpoint is reached. To train the network, we can label examples of places that already have (or should have) tourist offices and places that do (or should) not have tourist offices. These labels may be taken from the knowledge graph, or may be added manually. The GNN can then learn parameters \(\mathbf{w}\) and \(\mathbf{w'}\) that give the expected output for the labelled examples, which can subsequently be used to label other nodes.</p>

		<p>This GNN model is flexible and can be adapted in various ways&nbsp;<?php echo $references->cite("ScarselliGTHM09"); ?>: we may define neighbouring nodes differently, for example to include nodes for outgoing edges, or nodes one or two hops away; we may allow pairs of nodes to be connected by multiple edges with different vectors; we may consider transition and output functions with distinct parameters for each node; we may add states and outputs for edges; we may change the sum to another aggregation function; etc.</p>

		<figure id="fig-gnn">
			<img src="images/fig-gnn.svg" alt="On the left, a sub-graph of Figure&nbsp;24 highlighting the neighbourhood of Punta Arenas, where nodes are annotated with feature vectors (n_x) and hidden states at step t (h_x^{(t)}), and edges are annotated with feature vectors (a_{xy}); on the right, the GNN transition and output functions proposed by Scarselli et al. and an example for Punta Arenas (x = 1), where N(x) denotes the neighbouring nodes of x, f_w(·)\) denotes the transition function with parameters w and g_{w'}(·) denotes the output function with parameters w'" class="multi" />
			<div style="height:1.9em;">&nbsp;</div>
			<table id="tab-gnn">
				<tr>
					<td>\(\mathbf{h}_x^{(t)} \coloneqq\)</td>
					<td>\(\sum_{y \in \textrm{N}(x)} f_\mathbf{w}(\mathbf{n}_{x},\mathbf{n}_{y},\mathbf{a}_{yx},\mathbf{h}_{y}^{(t-1)})\)</td>
				</tr>
				<tr>
					<td>\(\mathbf{o}_x^{(t)} \coloneqq\)</td>
					<td>\(g_{\mathbf{w}'}(\mathbf{h}_x^{(t)},\mathbf{n}_x)\)</td>
				</tr>
				<tr>
					<td>\(\mathbf{h}_1^{(t)} \coloneqq\)</td>
					<td>\(f_\mathbf{w}(\mathbf{n}_{1},\mathbf{n}_{3},\mathbf{a}_{31},\mathbf{h}_{3}^{(t-1)})\)</td>
				</tr>
				<tr>
					<td></td>
					<td>\(+ f_\mathbf{w}(\mathbf{n}_{1},\mathbf{n}_{4},\mathbf{a}_{41},\mathbf{h}_{4}^{(t-1)})\)</td>
				</tr>
				<tr>
					<td>\(\mathbf{o}_1^{(t)} \coloneqq\)</td>
					<td>\(g_{\mathbf{w}'}(\mathbf{h}_1^{(t)},\mathbf{n}_1)\)</td>
				</tr>
				<tr>
					<td>\(\ldots\)</td>
					<td></td>
				</tr>
			</table>
			<div style="height:2.5em;">&nbsp;</div>
			<figcaption>On the left a sub-graph of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_3_1_Recursive_graph_neural_networks/figure_5_7.ttl"></a> highlighting the neighbourhood of Punta Arenas, where nodes are annotated with feature vectors (\(\mathbf{n}_x\)) and hidden states at step \(t\) (\(\mathbf{h}_x^{(t)}\)), and edges are annotated with feature vectors (\(\mathbf{a}_{xy}\)); on the right, the GNN transition and output functions proposed by <?php echo $references->citet("ScarselliGTHM09"); ?> and an example for Punta Arenas (\(x = 1\)), where \(\mathrm{N}(x)\) denotes the neighbouring nodes of \(x\), \(f_{\mathbf{w}}(\cdot)\) denotes the transition function with parameters \(\mathbf{w}\) and \(g_{\mathbf{w}'}(\cdot)\) denotes the output function with parameters \(\mathbf{w'}\)</figcaption>
		</figure>

		<div class="formal">
			<p>We now define a recursive graph neural network. We assume that the GNN accepts a directed vector-labelled graph as input (see Definition&nbsp;<?php echo ref("def:dvlg"); ?>).</p>

			<dl class="definition" id="def-recursive-graph-neural-network">
				<dt>Recursive graph neural network</dt>
				<dd>A <em>recursive graph neural network</em> (<em>RecGNN</em>) is a pair of functions \(\mathfrak{R} \coloneqq (\)<span class="sc">Agg</span>, <span class="sc">Out</span>\()\), such that (with \(a, b, c \in \mathbb{N}\)):
					<ul>
						<li><span class="sc">Agg</span>\(: \mathbb{R}^a \times 2^{(\mathbb{R}^a \times \mathbb{R}^b) \rightarrow \mathbb{N}} \rightarrow \mathbb{R}^a\)</li>
						<li><span class="sc">Out</span>\(: \mathbb{R}^a \rightarrow \mathbb{R}^c\)</li>
					</ul>
				</dd>
			</dl>

			<p>The function <span class="sc">Agg</span> computes a new feature vector for a node, given its previous feature vector and the feature vectors of the nodes and edges forming its neighbourhood; the function <span class="sc">Out</span> transforms the final feature vector computed by <span class="sc">Agg</span> for a node to the output vector for that node. We assume that \(a\) and \(b\) correspond to the dimensions of the input node and edge vectors, respectively, while \(c\) denotes the dimension of the output vector for each node. Given a RecGNN \(\mathfrak{R} = (\)<span class="sc">Agg</span>, <span class="sc">Out</span>\()\), a directed vector-labelled graph \(G = (V,E,F,\lambda)\), and a node \(u \in V\), we define the output vector assigned to node \(u\) in \(G\) by \(\mathfrak{R}\) (written \(\mathfrak{R}(G,u)\)) as follows. First let \(\mathbf{n}_u^{(0)} \coloneqq \lambda(u)\). For all \(i \geq 1\), let:</p>
			<p class="mathblock">\(\mathbf{n}_u^{(i)} \coloneqq\) <span class="sc">Agg</span> \(\left( \mathbf{n}_u^{(i-1)}, \{\!\!\{ (\mathbf{n}_v^{(i-1)},\lambda(v,u)) \mid (v,u) \in E \}\!\!\} \right) \)</p>
			<p>If \(j \geq 1\) is an integer such that \(\mathbf{n}_u^{(j)} = \mathbf{n}_u^{(j-1)}\) for all \(u \in V\), then \(\mathfrak{R}(G,u) \coloneqq\) <span class="sc">Out</span>\((\mathbf{n}_u^{(j)})\).</p>
			<p>In a RecGNN, the same aggregation function (<span class="sc">Agg</span>) is applied recursively until a fixpoint is reached, at which point an output function (<span class="sc">Out</span>}) creates the final output vector for each node. While in practice RecGNNs will often consider a static feature vector and a dynamic state vector&nbsp;<?php echo $references->cite("ScarselliGTHM09"); ?>, we can more concisely encode this as one vector, where part may remain static throughout the aggregation process representing input features, and part may be dynamically computed representing the state. In practice, <span class="sc">Agg</span> and <span class="sc">Out</span> are often based on parametric combinations of vectors, with the parameters learnt based on a sample of output vectors for labelled nodes.</p>

			<div class="example">
				<p>The aggregation function for the GNN of <?php echo $references->citet("ScarselliGTHM09"); ?> is given as:</p>
				<p class="mathblock"><span class="sc">Agg</span>\((\mathbf{n}_u,N) \coloneqq \sum_{(\mathbf{n}_v,\mathbf{a}_{vu})\in N}f_{\mathbf{w}}(\mathbf{n}_u,\mathbf{n}_v,\mathbf{a}_{vu})\)</p>
				<p>where \(f_{\mathbf{w}}(\cdot)\) is a contraction function with parameters \(\mathbf{w}\). The output function is defined as:</p>
				<p class="mathblock"><span class="sc">Out</span>\(\left( \mathbf{n}_u \right) \coloneqq g_{\textbf{w}'}(\mathbf{n}_u)\)</p>
				<p>where again \(g_{\mathbf{w}'}(\cdot)\) is a function with parameters \(\mathbf{w'}\). Given a set of nodes labelled with their expected output vectors, the parameters \(\mathbf{w}\) and \(\mathbf{w}'\) are learnt.</p>
			</div>

			<p>There are notable similarities between graph parallel frameworks (GPFs; see Definition&nbsp;<?php echo ref("def:gpf"); ?>) and RecGNNs. While we defined GPFs using separate <span class="sc">Msg</span> and <span class="sc">Agg</span> functions, this is not essential: conceptually they could be defined in a similar way to RecGNN, with a single <span class="sc">Agg</span> function that “pulls” information from its neighbours (we maintain <span class="sc">Msg</span> to more closely reflect how GPFs are defined/implemented in practice). The key difference between GPFs and GNNs is that in the former, the functions are defined by the user, while in the latter, the functions are generally learnt from labelled examples. Another difference arises from the termination condition present in GPFs, though often the GPF’s termination condition will – like in RecGNNs – reflect convergence to a fixpoint.</p>
		</div>

		<h4 id="sssec-convolutional-gnn" class="subsection">Non-recursive graph neural networks</h4>
		<p>GNNs can also be defined in a non-recursive manner, where a fixed number of layers are applied over the input in order to generate the output. A benefit of this approach is that we do not need to worry about convergence since the process is non-recursive. Also, each layer will often have independent parameters, representing different transformation steps. Naively, a downside is that adding many layers could give rise to a high number of parameters. Addressing this problem, a popular approach for non-recursive GNNs is to use convolutional neural networks.</p>
		<p>Convolutional neural networks (CNNs) have gained a lot of attention, in particular, for machine learning tasks involving images&nbsp;<?php echo $references->cite("KrizhevskySH17"); ?>. The core idea in the image setting is to train and apply small kernels (aka filters) over localised regions of an image using a convolution operator to extract features from that local region. When applied to all local regions, the convolution outputs a feature map of the image. Since the kernels are small, and are applied multiple times to different regions of the input, the number of parameters to train is reduced. Typically multiple kernels can thus be applied, forming multiple convolutional layers.</p>
		<p>One may note that in GNNs and CNNs, operators are applied over local regions of the input data. In the case of GNNs, the transition function is applied over a node and its neighbours in the graph. In the case of CNNs, the convolution is applied on a pixel and its neighbours in the image. Following this intuition, a number of <em>convolutional graph neural networks</em> (<em>ConvGNNs</em>)&nbsp;<?php echo $references->cite("BrunaZSL13,KipfW17,abs-1901-00596"); ?> have been proposed, where the transition function is implemented by means of convolutions. A key consideration for ConvGNNs is how regions of a graph are defined. Unlike the pixels of an image, nodes in a graph may have varying numbers of neighbours. This creates a challenge: a benefit of CNNs is that the same kernel can be applied over all the regions of an image, but this requires more careful consideration in the case of ConvGNNs since neighbourhoods of different nodes can be diverse. Approaches to address these challenges involve working with spectral (e.g.&nbsp;<?php echo $references->cite("BrunaZSL13,KipfW17"); ?>) or spatial (e.g.,&nbsp;<?php echo $references->cite("MontiBMRSB17"); ?>) representations of graphs that induce a more regular structure from the graph. An alternative is to use an attention mechanism&nbsp;<?php echo $references->cite("VelickovicCCRLB18"); ?> to <em>learn</em> the nodes whose features are most important to the current node.</p>
		
		<div class="formal">
			<p>Next we abstractly define a non-recursive graph neural network.</p>

			<dl class="definition" id="def-non-recursive-graph-neural-network">
				<dt>Non-recursive graph neural network</dt>
				<dd>A <em>non-recursive graph neural network</em> (NRecGNN) with \(l\) layers is an \(l\)-tuple of functions <span class="nobreak">\(\mathfrak{N} \coloneqq (\)<span class="sc">Agg</span>\(^{(1)},\ldots,\) <span class="sc">Agg</span>\(^{(l)} )\),</span> such that, for \(1 \leq k \leq l\) (with \(a_0, \ldots a_l, b \in \mathbb{N}\)), <span class="nobreak"><span class="sc">Agg</span>\(^{(k)}: \mathbb{R}^{a_{k-1}} \times 2^{(\mathbb{R}^{a_{k-1}} \times \mathbb{R}^b) \rightarrow \mathbb{N}} \rightarrow \mathbb{R}^{a_{k}}\)</span>.</dd>
			</dl>

			<p>Each function <span class="sc">Agg</span>\(^{(k)}\) (as before) computes a new feature vector for a node, given its previous feature vector and the feature vectors of the nodes and edges forming its neighbourhood. We assume that \(a_0\) and \(b\) correspond to the dimensions of the input node and edge vectors, respectively, where each function <span class="sc">Agg</span>\(^{(k)}\) for \(2 \leq k \leq l\) accepts as input node vectors of the same dimension as the output of the function <span class="sc">Agg</span>\(^{(k-1)}\). Given an NRecGNN \(\mathfrak{N} = (\) <span class="sc">Agg</span>\(^{(1)},\ldots,\) <span class="sc">Agg</span>\(^{(l)} )\), a directed vector-labelled graph \(G = (V,E,F,\lambda)\), and a node \(u \in V\), we define the output vector assigned to node \(u\) in \(G\) by \(\mathfrak{N}\) (written \(\mathfrak{N}(G,u)\)) as follows. First let \(\mathbf{n}_u^{(0)} \coloneqq \lambda(u)\). For all \(i \geq 1\), let:</p>
			<p class="mathblock">\(\mathbf{n}_u^{(i)} \coloneqq\) <span class="sc">Agg</span>\(^{(i)} \left( \mathbf{n}_u^{(i-1)}, \{\!\!\{ (\mathbf{n}_v^{(i-1)},\lambda(v,u)) \mid (v,u) \in E \}\!\!\} \right) \)</p>
			<p>Then \(\mathfrak{N}(G,u) \coloneqq \mathbf{n}_u^{(l)}\).</p>
			<p>In an \(l\)-layer NRecGNN, a different aggregation function can be applied at each step (i.e., in each layer), up to a fixed number of steps \(l\). We do not consider a separate <span class="sc">Out</span> function as it can be combined with the final aggregation function <span class="sc">Agg</span>\(^{(l)}\). When the aggregation functions use a convolutional operator based on kernels learned from labelled examples, we call the result a <em>convolutional graph neural network</em> (<em>ConvGNN</em>). We refer to the survey by <?php echo $references->citet("abs-1901-00596"); ?> for discussion of ConvGNNs proposed in the literature.</p>
			<p>We have considered GNNs that define the neighbourhood of a node based on its incoming edges. These definitions can be adapted to also consider outgoing neighbours by either adding inverse edges to the directed vector-labelled graph in pre-processing, or by adding outgoing neighbours as arguments to the <span class="sc">Agg</span>\((\cdot)\) function. More generally, GNNs (and indeed GPFs) relying solely on the neighbourhood of each node have limited expressivity in terms of their ability to distinguish nodes and graphs&nbsp;<?php echo $references->cite("XuHLJ19"); ?>; for example, <?php echo $references->citet("BarceloKMPRS20"); ?> show that such NRecGNNs have a similar expressiveness for classifying nodes as the \(\mathcal{ALCQ}\) Description Logic discussed in Section&nbsp;<?php echo ref("sssec:dls"); ?>. More expressive GNN variants have been proposed that allow the aggregation functions to access and update a globally shared vector&nbsp;<?php echo $references->cite("BarceloKMPRS20"); ?>. We refer to the papers by <?php echo $references->citet("XuHLJ19"); ?> and <?php echo $references->citet("BarceloKMPRS20"); ?> for further discussion.</p>
		</div>
		</section>

		<section id="ssec-symlearn" class="section">
		<h3>Symbolic Learning</h3>
		<p>The supervised techniques discussed thus far – namely knowledge graph embeddings and graph neural networks – learn numerical models over graphs. However, such models are often difficult to explain or understand. For example, taking the graph of Figure&nbsp;<?php echo ref("fig:airports"); ?>, knowledge graph embeddings might predict the edge <?php echo gedge("SCL","flight","ARI"); ?> as being highly plausible, but they will not provide an interpretable model to help understand why this is the case: the reason for the result may lie in a matrix of parameters learnt to fit a plausibility score on training data. Such approaches also suffer from the <em>out-of-vocabulary</em> problem, where they are unable to provide results for edges involving previously unseen nodes or edges; for example, if we add an edge <?php echo gedge("SCL","flight","CDG"); ?>, where <span class="gnode">CDG</span> is new to the graph, a knowledge graph embedding will not have the entity embedding for <span class="gnode">CDG</span> and would need to be retrained in order to estimate the plausibility of an edge <?php echo gedge("CDG","flight","SCL"); ?>.</p>

		<figure id="fig-airports">
			<img src="images/fig-airports.svg" alt="An incomplete directed edge-labelled graph describing flights between airports"/>
			<figcaption>A directed edge-labelled graph describing flights between airports <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_5_Inductive_Knowledge/5_4_Symbolic_Learning/figure_5_8.ttl"></a></figcaption>
		</figure>

		<p>An alternative (sometimes complementary) approach is to adopt <em>symbolic learning</em> in order to learn <em>hypotheses</em> in a symbolic (logical) language that “explain” a given set of positive and negative edges. These edges are typically generated from the knowledge graph in an automatic manner (similar to the case of knowledge graph embeddings). The hypotheses then serve as interpretable models that can be used for further deductive reasoning. Given the graph of Figure&nbsp;<?php echo ref("fig:airports"); ?>, we may, for example, learn the rule <?php echo sedge("?x","flight",NULL,"?y","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?y","flight",NULL,"?x","gvar"); ?> from observing that <span class="gelab">flight</span> routes tend to be return routes. Alternatively, rather than learn rules, we might learn a DL axiom from the graph stating that airports are either domestic, international, or both: <code>Airport</code> \(\sqsubseteq\) <code>DomesticAirport</code> \(\sqcup\) <code>InternationalAirport</code>. Such rules and axioms can then be used for deductive reasoning, and offer an interpretable model for new knowledge that is entailed/predicted; for example, from the aforementioned rule for return flights, one can interpret why a novel edge <?php echo gedge("SCL","flight","ARI"); ?> is predicted. This further offers domain experts the opportunity to verify the models – e.g., the rules and axioms – derived by such processes. Finally, rules/axioms are quantified (<em>all</em> flights have a return flight, <em>all</em> airports are domestic or international, etc.), so they can be applied to unseen examples (e.g., with the aforementioned rule, we can derive <?php echo gedge("CDG","flight","SCL"); ?> from a new edge <?php echo gedge("SCL","flight","CDG"); ?> with the unseen node <span class="gnode">CDG</span>).</p>
		<p>In this section, we discuss two forms of symbolic learning: <em>rule mining</em>, which learns rules, and <em>axiom mining</em>, which learns other forms of logical axioms.</p>

		<h4 id="sssec-rule-mining" class="subsection">Rule mining</h4>
		<p>Rule mining, in the general sense, refers to discovering meaningful patterns in the form of rules from large collections of background knowledge. In the context of knowledge graphs, we assume a set of positive and negative edges as given. Typically positive edges are observed edges (i.e., those given or entailed by a knowledge graph) while negative edges are defined according to a given assumption of completeness (discussed later). The goal of rule mining is to identify new rules that entail a high ratio of positive edges from other positive edges, but entail a low ratio of negative edges from positive edges. The types of rules considered may vary from more simple cases, such as <?php echo sedge("?x","flight",NULL,"?y","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?y","flight",NULL,"?x","gvar"); ?> mentioned previously, to more complex rules, such as <?php echo sedge("?x","capital",NULL,"?y","gvar"); ?><?php echo esource(); ?><span class="edge">nearby</span><?php echo etipr(); ?><span class="gvar">?z</span><?php echo esource(); ?><span class="edge">type</span><?php echo etipr(); ?><span class="gnode">Airport</span> \(\Rightarrow\) <span class="gvar">?z</span><?php echo esource(); ?><span class="edge">type</span><?php echo etipr(); ?><span class="gnode">International&nbsp;Airport</span>, based on observing in the graph that airports near capitals tend to be international airports; or <img class="inside" src="images/fig-inline-rule.svg" alt="dom flight rule premise" style="margin-top:.5em;margin-bottom:.2em;margin-left:1.8em;margin-right:1.8em;vertical-align:middle;position:relative;"/> \(\Rightarrow\) <?php echo sedge("?x","domestic&nbsp;flight",NULL,"?y","gvar"); ?>, indicating that flights within the same country denote domestic flights (as seen previously in Section&nbsp;<?php echo ref("sec:rules"); ?>).</p>
		<p>Per the example inferring that airports near capital cities are international airports, rules are not assumed to hold in all cases, but rather are associated with measures of how well they conform to the positive and negative edges. In more detail, we call the edges entailed by a rule and the set of positive edges (not including the entailed edge itself), the <em>positive entailments</em> of that rule. The number of entailments that are positive is called the <em>support</em> for the rule, while the ratio of a rule’s entailments that are positive is called the <em>confidence</em> for the rule&nbsp;<?php echo $references->cite("SuchanekLBW19"); ?>. Support and confidence indicate, respectively, the number and ratio of entailments “confirmed” to be true for the rule, where the goal is to identify rules that have both high support and high confidence. Techniques for rule mining in relational settings have long been explored in the context of <em>Inductive Logic Programming</em> (<em>ILP</em>)&nbsp;<?php echo $references->cite("DeRaedt08"); ?>. However, knowledge graphs present novel challenges due to the scale of the data and the frequent assumption of incomplete data (OWA), where dedicated techniques have been proposed to address these issues&nbsp;<?php echo $references->cite("GalarragaTHS13"); ?>.</p>
		<p>When dealing with an incomplete knowledge graph, it is not immediately clear how to define negative edges. A common heuristic – also used for knowledge graph embeddings – is to adopt a Partial Completeness Assumption (PCA)&nbsp;<?php echo $references->cite("GalarragaTHS13"); ?>, which considers the set of positive edges to be those contained in the data graph, and the set of negative examples to be the set of all edges <?php echo gedge("\(x\)","\(p\)","\(y\)"); ?> not in the graph but where there exists a node <span class="gnode">\(y'\)</span> such that <?php echo gedge("\(x\)","\(p\)","\(y'\)"); ?> is in the graph. Taking Figure&nbsp;<?php echo ref("fig:airports"); ?>, an example of a negative edge under PCA would be <?php echo gedge("SCL","flight","ARI"); ?> (given the presence of <?php echo gedge("SCL","flight","LIM"); ?>); conversely, <?php echo gedge("SCL","domestic&nbsp;flight","ARI"); ?> is neither positive nor negative. The PCA confidence measure is then the ratio of the support to the number of entailments in the positive or negative set&nbsp;<?php echo $references->cite("GalarragaTHS13"); ?>. For example, the support for the rule <?php echo sedge("?x","domestic&nbsp;flight",NULL,"?y","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?y","domestic&nbsp;flight",NULL,"?x","gvar"); ?> is \(2\) (since it entails <?php echo gedge("IQQ","domestic&nbsp;flight","ARI"); ?> and <?php echo gedge("ARI","domestic&nbsp;flight","IQQ"); ?> in the graph, which are thus positive edges), while the confidence is \(\frac{2}{2} = 1\) (noting that <?php echo gedge("SCL","domestic&nbsp;flight","ARI"); ?>, though entailed, is neither positive nor negative, and is thus ignored by the measure). The support for the rule <?php echo sedge("?x","flight",NULL,"?y","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?y","flight",NULL,"?x","gvar"); ?> is analogously 4, while the confidence is \(\frac{4}{5} = 0.8\) (noting that <?php echo gedge("SCL","flight","ARI"); ?> is a negative edge).</p>
		<p>The goal then, is to find rules satisfying given support and confidence thresholds. An influential rule-mining system for graphs is AMIE&nbsp;<?php echo $references->cite("GalarragaTHS13,GalarragaTHS15"); ?>, which adopts the PCA measure of confidence, and builds rules in a top-down fashion&nbsp;<?php echo $references->cite("SuchanekLBW19"); ?> starting with rule heads of the form \(\Rightarrow\) <?php echo sedge("?x","country",NULL,"?y","gvar"); ?>. For each such rule head (one for each edge label), three types of <em>refinements</em> are considered, each of which adds a new edge to the body of the rule. This new edge takes an edge label from the graph and may otherwise use <em>fresh variables</em> not appearing previously in the rule, <em>existing variables</em> that already appear in the rule, or nodes from the graph. The three refinements may then:</p>
		<ol>
			<li>add an edge with one existing variable and one fresh variable; for example, refining the aforementioned rule head might give: <?php echo sedge("?z","flight",NULL,"?x","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?x","country",NULL,"?y","gvar"); ?>;</li>
			<li>add an edge with an existing variable and a graph node; for example, refining the above rule might give: <span class="gnode">Domestic&nbsp;Airport</span><?php echo etipl(); ?><span class="edge">type</span><?php echo esource(); ?><?php echo sedge("?z","flight",NULL,"?x","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?x","country",NULL,"?y","gvar"); ?>;</li>
			<li>add an edge with two existing variables; for example, refining the above rule might give: <img class="inside" src="images/rule-mining-domairport.svg" alt="dom airport rule premise" style="margin-right:2.1em;"/> \(\Rightarrow\) <?php echo sedge("?x","country",NULL,"?y","gvar"); ?>.</li>
		</ol>
		<p>These refinements can be combined arbitrarily, which gives rise to a potentially exponential search space, where rules meeting given thresholds for support and confidence are maintained. To improve efficiency, the search space can be pruned; for example, these three refinements always decrease support, so if a rule does not meet the support threshold, there is no need to explore its refinements. Further restrictions are imposed on the types of rules generated. First, only rules up to a certain fixed size are considered. Second, a rule must be <em>closed</em>, meaning that each variable appears in at least two edges of the rule, which ensures that rules are <em>safe</em>, meaning that each variable in the head appears in the body; for example, the rules produced by the first and second refinements in the example are neither closed (variable <span class="gvar">y</span> appears once) nor safe (variable <span class="gvar">y</span> appears only in the head).<?php echo footnote("Safe rules like ". sedge("?x","capital",NULL,"?y","gvar") . esource() ."<span class=\"edge\">nearby</span>". etipr() ."<span class=\"gvar\">?z</span>". esource() ."<span class=\"edge\">type</span>". etipr() ."<span class=\"gnode\">Airport</span> \\(\\Rightarrow\\) <span class=\"gvar\">?z</span>". esource() ."<span class=\"edge\">type</span>". etipr() ."<span class=\"gnode\">International&nbsp;Airport</span> are not closed as <span class=\"gvar\">?x</span> appears only in one edge. The condition that rules are closed is strictly stronger than the safety condition."); ?> The third refinement is thus applied until a rule is closed. For further discussion of possible optimisations based on pruning and indexing, we refer to the paper&nbsp;<?php echo $references->cite("GalarragaTHS15"); ?>.</p>
		<p>Later works have built on these techniques for mining rules from knowledge graphs. <?php echo $references->citet("Gad-ElrabSUW16"); ?> propose a method to learn non-monotonic rules – rules with negated edges in the body – in order to capture exceptions to base rules; for example, the rule <img class="inside" src="images/rule-mining-not-international.svg" alt="not international rule premise" style="margin-right:2.1em;" /> \(\Rightarrow\) <?php echo sedge("?x","country",NULL,"?y","gvar"); ?> may be learnt, indicating that flights are within the same country <em>except</em> when the (departure) airport is international, where the exception is shown dotted and we use \(\neg\) to negate an edge (representing an exception). The RuLES system&nbsp;<?php echo $references->cite("HoSGKW18"); ?> – which is also capable of learning non-monotonic rules – proposes to mitigate the limitations of the PCA heuristic by extending the confidence measure to consider the plausibility scores of knowledge graph embeddings for entailed edges not appearing in the graph. Where available, explicit statements about the completeness of the knowledge graph (such as expressed in shapes; see Section&nbsp;<?php echo ref("sssec:validating-schema"); ?>) can be used in lieu of PCA for identifying negative edges. Along these lines, CARL&nbsp;<?php echo $references->cite("TanonSRMW17"); ?> exploits additional knowledge about the cardinalities of relations to refine the set of negative examples and the confidence measure for candidate rules. Alternatively, where available, ontologies can be used to derive logically-certain negative edges under OWA through, for example, disjointness axioms. The system proposed by d’Amato et al.&nbsp;<?php echo $references->cite("dAmatoTM16,dAmatoSTMG16"); ?> leverages ontologically-entailed negative edges for determining the confidence of rules generated through an evolutionary algorithm.</p>
		<p>While the previous works involve discrete expansions of candidate rules for which a fixed confidence scoring function is applied, another line of research is on a technique called <em>differentiable rule mining</em>&nbsp;<?php echo $references->cite("Rocktaschel017,YangYC17,SadeghianADW19"); ?>, which allows end-to-end learning of rules. The core idea is that the joins in rule bodies can be represented as matrix multiplication. More specifically, we can represent the relations of an edge label \(p\) by the adjacency matrix \(\mathbf{A}_p\) (of size \(|V| \times |V|\)) such that the value on the \(i\)<sup>th</sup> row of the \(j\)<sup>th</sup> column is \(1\) if there is an edge labelled \(p\) from the \(i\)<sup>th</sup> entity to the \(j\)<sup>th</sup> entity; otherwise the value is \(0\). Now we can represent a join in a rule body as matrix multiplication; for example, given <?php echo sedge("?x","domestic&nbsp;flight",NULL,"?y","gvar"); ?><?php echo etipl(); ?><span class="edge">country</span><?php echo esource(); ?><span class="gvar">?z</span> \(\Rightarrow\) <?php echo sedge("?x","country",NULL,"?z","gvar"); ?>, we can denote the body by the matrix multiplication \(\mathbf{A}\)<sub><span class="gelab">df.</span></sub>\(\mathbf{A}\)<sub><span class="gelab">c.</span></sub>, which gives an adjacency matrix representing entailed <span class="gelab">country</span> edges, where we should expect the \(1\)’s in \(\mathbf{A}\)<sub><span class="gelab">df.</span></sub>\(\mathbf{A}\)<sub><span class="gelab">c.</span></sub> to be covered by the head’s adjacency matrix \(\mathbf{A}\)<sub><span class="gelab">c.</span></sub>. Since we are given adjacency matrices for all edge labels, we are left to learn confidence scores for individual rules, and to learn rules (of varying length) with a threshold confidence. Along these lines, NeuralLP&nbsp;<?php echo $references->cite("YangYC17"); ?> uses an <em>attention mechanism</em> to select a variable-length sequence of edge labels for path-like rules of the form <?php echo sedge("?x","p<sub>\\(1\\)</sub>",NULL,"y<sub>\\(1\\)</sub>","gvar"); ?><?php echo esource(); ?><span class="edge">p<sub>\(2\)</sub></span><?php echo etipr(); ?>…<?php echo esource(); ?><span class="edge">p<sub>\(n\)</sub></span><?php echo etipr(); ?><?php echo sedge("y<sub>\\(n\\)</sub>","p<sub>\\(n+1\\)</sub>",NULL,"?z","gvar"); ?> \(\Rightarrow\) <?php echo sedge("?x","p",NULL,"?z","gvar"); ?>, for which confidences are likewise learnt. DRUM&nbsp;<?php echo $references->cite("SadeghianADW19"); ?> also learns path-like rules, where, observing that some edge labels are more/less likely to follow others in the rules – for example, <span class="gelab">flight</span> will not be followed by <span class="gelab">capital</span> in the graph of Figure&nbsp;<?php echo ref("fig:chileTransport"); ?> as the join will be empty – the system uses bidirectional recurrent neural networks (a popular technique for learning over sequential data) to learn sequences of relations for rules, and their confidences. These differentiable rule mining techniques are, however, currently limited to learning path-like rules.</p>

		<h4 id="sssec-axiom-mining" class="subsection">Axiom mining</h4>
		<p>More general forms of axioms beyond rules – expressed in logical languages such as DLs (see Section&nbsp;<?php echo ref("sssec:dls"); ?>) – can be mined from knowledge graphs. We can divide these approaches into two: those mining specific axioms and more general axioms.</p>
		<p>Among systems mining specific types of axioms, disjointness axioms are a popular target; for example, <code>DomesticAirport</code> \(\sqcap\) <code>InternationalAirport</code> \(\equiv \bot\) states that the two classes are disjoint by equivalently stating that the intersection of the two classes is equivalent to the empty class, or in simpler terms, no node can be simultaneously of type <span class="gnode">Domestic Airport</span> and <span class="gnode">International Airport</span>. The system proposed by <?php echo $references->citet("Volker2015"); ?> extracts disjointness axioms based on (negative) <em>association rule mining</em>&nbsp;<?php echo $references->cite("Agrawal93"); ?>, which finds pairs of classes where each has many instances in the knowledge graph but there are relatively few (or no) instances of both classes. <?php echo $references->citet("TopperKS12"); ?> rather extract disjointness for pairs of classes that have a cosine similarity below a fixed threshold. For computing this cosine similarity, class vectors are computed using a TF–IDF analogy, where the “document” of each class is constructed from all of its instances, and the “terms” of this document are the properties used on the class instances (preserving multiplicities). While the previous two approaches find disjointness constraints between named classes (e.g., <em>city</em> is disjoint with <em>airport</em>), <?php echo $references->citet("Rizzo2017"); ?>, <?php echo $references->citet("RizzodF21"); ?> propose an approach that can capture disjointness constraints between class descriptions (e.g., <em>city without an airport nearby</em> is disjoint with <em>city that is the capital of a country</em>). The approach first clusters similar nodes of the knowledge base. Next, a <em>terminological cluster tree</em> is extracted, where each leaf node indicates a cluster extracted previously, and each internal (non-leaf) node is a class definition (e.g., <em>cities</em>) where the left child is either a cluster having all nodes in that class or a sub-class description (e.g., <em>cities without airports</em>) and the right child is either a cluster having no nodes in that class or a disjoint-class description (e.g., <em>non-cities with events</em>). Finally, candidate disjointness axioms are proposed for pairs of class descriptions in the tree that are not entailed to have a sub-class relation.</p>
		<p>Other systems propose methods to learn more general axioms. One of the first proposals in this direction is the DL-FOIL system&nbsp;<?php echo $references->cite("FanizzidE08,RizzoFd20"); ?>, which is based on algorithms for <em>class learning</em> (aka <em>concept learning</em>), whereby given a set of positive nodes and negative nodes, the goal is to find a logical class description that divides the positive and negative sets. For example, given \(\{\)<span class="gnode">Iquique</span>, <span class="gnode">Arica</span>\(\}\) as the positive set and \(\{\)<span class="gnode">Santiago</span>\(\}\) as the negative set, we may learn a (DL) class description \(\exists\)<code>nearby</code>.<code>Airport</code> \(\sqcap \neg(\exists\) <code>capital</code>\(^-.\top)\), denoting entities near to an airport that are not capitals, of which all positive nodes are instances and no negative nodes are instances. Such class descriptions are learnt in an analogous manner to how aforementioned systems like AMIE learn rules, with a refinement operator used to move from more general classes to more specific classes (and vice-versa), a confidence scoring function, and a search strategy. Another prominent such system is DL-Learner&nbsp;<?php echo $references->cite("BuhmannLW16"); ?>, which system further supports learning more general axioms through a scoring function that uses count queries to determine what ratio of expected edges – edges that would be entailed were the axiom true – are indeed found in the graph; for example, to score the axiom \(\exists\)<code>flight</code>\(^{-}\).<code>DomesticAirport</code> \(\sqsubseteq\) <code>InternationalAirport</code> over Figure&nbsp;<?php echo ref("fig:airports"); ?>, we can use a graph query to count how many nodes have incoming flights from a domestic airport (there are \(3\)), and how many nodes have incoming flights from a domestic airport <em>and</em> are international airports (there is \(1\)), where the greater the difference between both counts, the weaker the evidence for the axiom.</p>

		<h4 id="sssec-hypothesis-mining" class="subsection">Hypothesis mining</h4>
		<p>We now provide some abstract formal definitions for the tasks of <em>rule mining</em> and <em>axiom mining</em> over graphs, which we generically refer to as <em>hypothesis mining</em>.</p>

		<div class="formal">
			<p>First we introduce <em>hypothesis induction</em>: a task that captures a more abstract (ideal) case for hypothesis mining. For simplicity, we focus on directed edge-labelled graphs. With a slight abuse of notation, we may interpret a set of edges \(E\) as the graph with precisely those edges and with no nodes or labels without edges. We may also interpret an edge \(e\) as the graph formed by \(\{ e \}\).</p>

			<dl class="definition" id="def-hypothesis-induction">
				<dt>Hypothesis induction</dt>
				<dd>The task of <em>hypothesis induction</em> assumes a particular graph entailment relation \(\models_\Phi\) (see Definition&nbsp;<?php echo ref("def:ent"); ?>; hereafter simply \(\models\)). Given <em>background knowledge</em> in the form of a knowledge graph \(G\) (a directed edge-labelled graph, possibly extended with rules or ontologies), a set of <em>positive edges</em> \(E^{+}\) such that \(G\) does not entail any edge in \(E^{+}\) (i.e., for all \(e^{+} \in E^{+}\), \(G \not\models e^{+}\)) and \(E^{+}\) does not contradict \(G\) (i.e., there is a model of \(G \cup E^{+}\)), and a set of <em>negative edges</em> \(E^{-}\) such that \(G\) does not entail any edge in \(E^-\) (i.e., for all \(e^{-} \in E^{-}\), \(G \not\models e^{-}\)), the task is to find a set of <em>hypotheses</em> (i.e., a set of directed edge-labelled graphs) \(\Psi\) such that:
					<ul>
						<li>\(G \not\models \psi\) for all \(\psi \in \Psi\) (the background knowledge does not entail any hypothesis directly);</li>
						<li>\(G \cup \Psi^* \models E^{+}\) (the background knowledge and hypotheses together entail all positive edges);</li>
						<li>for all \(e^{-} \in E^{-}\), \(G \cup \Psi^* \not\models e^{-}\) (the background knowledge and hypotheses together do not entail any negative edge);</li>
						<li>\(G \cup \Psi^* \cup E^{+}\) has a model (the background knowledge, hypotheses and positive edges taken together do not contain a contradiction);</li>
						<li>for all \(e^{+} \in E^{+}\), \(\Psi^* \not\models e^{+}\) (the hypotheses alone do not entail a positive edge).</li>
					</ul>
				where by \(\Psi^* \coloneqq \cup_{\psi \in \Psi} \psi\) we denote the union of all graphs in \(\Psi\).</dd>
			</dl>

			<div class="example">
				<p>Let us assume ontological entailment \(\models\) with semantic conditions \(\Phi\) as defined in Tables&nbsp;<?php echo ref("tab:ontEqIneq"); ?>–<?php echo ref("tab:ontClass"); ?>. Given the graph of Figure&nbsp;<?php echo ref("fig:airports"); ?> as the background knowledge \(G\), along with:</p>
				<ul>
					<li>a set of positive edges \(E^{+} = \{ \)<?php echo gedge("SCL","flight","ARI"); ?>, <?php echo gedge("SCL","domestic flight","ARI"); ?>\( \}\), and</li>
					<li>a set of negative edges \(E^{-} = \{ \)<?php echo gedge("ARI","flight","LIM"); ?>, <?php echo gedge("SCL","domestic flight","LIM"); ?>\( \}\),</li>
				</ul>
				<p>then a set of hypotheses \(\Psi = \{ \)<?php echo gedge("flight","type","Symmetric"); ?>, <?php echo gedge("domestic flight","type","Symmetric"); ?>\( \}\) are not entailed by \(G\), entail all positive edges in \(E^{+}\) and no negative edges in \(E^{-}\) when combined with \(G\), do not contradict \(G \cup E^{+}\), and do not entail a positive edge without \(G\). Thus \(\Psi\) satisfies the conditions for hypothesis induction.</p>
			</div>

			<p>This task represents a somewhat idealised case. Often there is no set of positive edges distinct from the background knowledge itself. Furthermore, hypotheses not entailing a few positive edges, or entailing a few negative edges, may still be useful. The task of <em>hypothesis mining</em> rather accepts as input the background knowledge \(G\) and a set of negative edges \(E^{-}\) (such that for all \(e^{-} \in E^{-}\), \(G \not\models e^{-}\)), and attempts to <em>score</em> individual hypotheses \(\psi\) (such that \(G \not\models \psi\)) per their ability to “explain” \(G\) while minimising the number of elements of \(E^{-}\) entailed by \(G\) and \(\psi\). We can now abstractly define the task of hypothesis mining.</p>

			<dl class="definition" id="def-hypothesis-mining">
				<dt>Hypothesis mining</dt>
				<dd>Given a knowledge graph \(G\), a set of negative edges \(E^{-}\), a scoring function \(\sigma\), and a threshold \(\textsf{min}_{\sigma}\), the goal of <em>hypothesis mining</em> is to identify a set of hypotheses \(\{ \psi \mid G \not\models \psi\text{ and }\sigma(\psi,G,E^{-}) \geq \textsf{min}_{\sigma} \}\).</dd>
			</dl>

			<p>There are two scoring functions that are frequently used for \(\sigma\) in the literature: <em>support</em> and <em>confidence</em>.</p>

			<dl class="definition" id="def-hypothesis-support-and-confidence">
				<dt>Hypothesis support and confidence</dt>
				<dd>Given a knowledge graph \(G = (V,E,L)\) and a hypothesis \(\psi\), the <em>positive support</em> of \(\psi\) is defined as:
				\[ \sigma^{+}(\psi,G) \coloneqq |\{ e \in E \mid G' \not\models e \text{ and }G' \cup \psi \models e \}| \] 
				where \(G'\) denotes \(G\) with the edge \(e\) removed. Further given a set of negative edges \(E^{-}\), the <em>negative support</em> of \(\psi\) is defined as:
				\[ \sigma^{-}(\psi,G,E^{-}) \coloneqq |\{ e^{-} \in E^{-} \mid G \cup \psi \models e^{-} \}| \] 
				Finally, the <em>confidence</em> of \(\psi\) is defined as \(\sigma^\pm(\psi,G,E^{-}) \coloneqq \frac{\sigma^{+}(\psi,G)}{\sigma^{+}(\psi,G) + \sigma^{-}(\psi,G,E^{-})}\).</dd>
			</dl>

			<p>We have yet to define how the set of negative edges are defined, which, in the context of a knowledge graph \(G\), depends on which assumption is applied:</p>
			<ul>
				<li><em>Closed world assumption (CWA)</em>: For any (positive) edge \(e\), \(G \not\models e\) if and only if \(G \models \neg e\). Under CWA, any edge \(e\) not entailed by \(G\) can be considered a negative edge.</li>
				<li><em>Open world assumption</em>: For a (positive) edge \(e\), \(G \not\models e\) does not necessarily imply \(G \models \neg e\). Under OWA, the negation of an edge must be entailed by \(G\) for it to be considered negative.</li>
				<li><em>Partial completeness assumption (PCA)</em>: If there exists an edge \((s,p,o)\) such that \(G \models (s,p,o)\), then for all \(o'\) such that \(G \not\models (s,p,o')\), it is assumed that \(G \models \neg(s,p,o')\). Under PCA, if \(G\) entails some outgoing edge(s) labelled \(p\) from a node \(s\), then such edges are assumed to be complete, and any edge \((s,p,o')\) not entailed by \(G\) can be considered a negative edge.</li>
			</ul>
			<p>Knowledge graphs are generally incomplete – in fact, one of the main applications of hypothesis mining is to try to improve the completeness of the knowledge graph – and thus it would appear unwise to assume that any edge that is not currently entailed is false/negative. We can thus rule out CWA. Conversely, under OWA, potentially few (or no) negative edges might be entailed by the given ontologies/rules, and thus hypotheses may end up having low negative support despite entailing many edges that do not make sense in practice. Hence the PCA can be adopted as a heuristic to increase the number of negative edges and apply more sensible scoring of hypotheses. We remark that one can adapt PCA to define negative triples by changing the subject or predicate instead of the object.</p>
			<p>Different implementations of hypothesis mining may consider different logical languages. Rule mining, for example, mines hypotheses expressed either as monotonic rules (with positive edges) or non-monotonic edges (possibly with negated edges). On the other hand, axiom mining considers hypotheses expressed in a logical language such as Description Logics. Particular implementations may, for practical reasons, impose further syntactic restrictions on the hypotheses generated, such as to impose thresholds on their length, on the symbols they use, or on other structural properties (such as “closed rules” in the case of the AMIE rule mining system&nbsp;<?php echo $references->cite("GalarragaTHS13"); ?>; see Section&nbsp;<?php echo ref("ssec:symlearn"); ?>). Systems may further implement different search strategies for hypotheses. Systems such as DL-FOIL&nbsp;<?php echo $references->cite("FanizzidE08,RizzoFd20"); ?>, AMIE&nbsp;<?php echo $references->cite("GalarragaTHS13"); ?>, RuLES&nbsp;<?php echo $references->cite("HoSGKW18"); ?>, CARL&nbsp;<?php echo $references->cite("TanonSRMW17"); ?>, DL-Learner&nbsp;<?php echo $references->cite("BuhmannLW16"); ?>, etc., propose <em>discrete mining</em> that recursively generates candidate formulae through refinement/genetic operators that are then scored and checked for threshold criteria. On the other hand, systems such as NeuralLP&nbsp;<?php echo $references->cite("YangYC17"); ?> and DRUM&nbsp;<?php echo $references->cite("SadeghianADW19"); ?> apply <em>differentiable mining</em> that allows for learning (path-like) rules and their scores in a more continuous fashion (e.g., using gradient descent). We refer to Section&nbsp;<?php echo ref("ssec:symlearn"); ?> for further discussion and examples of such techniques for mining hypotheses.</p>
		</div>
		</section>
	</section>
